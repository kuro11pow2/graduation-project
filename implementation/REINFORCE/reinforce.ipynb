{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym      # environment 라이브러리\r\n",
    "import torch    # pytorch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "from torch.distributions import Categorical\r\n",
    "\r\n",
    "# Hyperparameters\r\n",
    "learning_rate = 0.0005\r\n",
    "gamma = 0.98"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# pytorch의 nn.Module 클래스를 상속해서 만든다. \r\n",
    "class Policy(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Policy, self).__init__()\r\n",
    "        # 에피소드 완료되기 전까지 임시로 데이터를 저장할 변수\r\n",
    "        self.data = []\r\n",
    "\r\n",
    "        # state가 4개이기 때문에 feature vector는 4차원이다. 은닉층은 128차원으로 잡았다.\r\n",
    "        # nn.Linear()는 fully connected이다. 즉, 4차원에서 128차원으로 가는 선형변환이다.\r\n",
    "        self.fc1 = nn.Linear(4, 128)\r\n",
    "        self.fc2 = nn.Linear(128, 2)\r\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "    # 정방향 전달하는 모든 호출에서 수행할 계산을 정의한다.\r\n",
    "    def forward(self, x):\r\n",
    "        # 4차원 feature vector를 128차원으로 변환하고 ReLU()를 취한 값을 tensor로 return한다.\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        # 128차원을 2차원으로 바꾸고 softmax()를 취한 값을 tensor로 return한다.\r\n",
    "        x = F.softmax(self.fc2(x), dim=0)\r\n",
    "        return x\r\n",
    "\r\n",
    "    # 데이터 쌓아두기\r\n",
    "    def put_data(self, item):\r\n",
    "        self.data.append(item)\r\n",
    "\r\n",
    "    # 네트워크 학습시키기\r\n",
    "    def train_net(self):\r\n",
    "        R = 0\r\n",
    "        self.optimizer.zero_grad()\r\n",
    "        # 거꾸로 보는 이유는 return을 쉽게 계산하기 위해서이다. \r\n",
    "        # 예를 들어, 에피소드가 100스텝이라고 하자. \r\n",
    "        # 99스텝에서 reward를 구하면 이게 return이다.\r\n",
    "        # 98스텝에서 reward를 구하면, 99스텝에서 계산한 return에 감마를 곱하고 reward를 더해서 return을 구할 수 있다.\r\n",
    "        for r, prob in self.data[::-1]:\r\n",
    "            # return을 cumulative하게 계산해 나간다.\r\n",
    "            R = r + gamma * R\r\n",
    "            # loss는 -log(pi) * R 로 정의한다.\r\n",
    "            loss = -torch.log(prob) * R\r\n",
    "            # autograd가 backpropagation을 자동으로 처리한다.\r\n",
    "            # model의 parameter에 대한 loss의 gradient를 계산한다.\r\n",
    "            loss.backward()\r\n",
    "        # single optimization step을 수행하여 parameter를 update함.\r\n",
    "        self.optimizer.step()\r\n",
    "        # 이미 학습한 데이터를 제거한다.\r\n",
    "        self.data = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def main():\r\n",
    "    # environment를 생성한다. \r\n",
    "    env = gym.make('CartPole-v1')\r\n",
    "    # Policy 클래스의 인스턴스를 생성한다.\r\n",
    "    pi = Policy()\r\n",
    "    score = 0.0\r\n",
    "    print_interval = 20\r\n",
    "\r\n",
    "    # episode를 만 번 시뮬레이션 한다.\r\n",
    "    for n_epi in range(10000):\r\n",
    "        # env를 처음 상태로 초기화함과 동시에 observation 결과, 즉 state를 돌려준다.\r\n",
    "        # CartPole의 경우 state는 네 개의 실수로 구성되어 있다. \r\n",
    "        # CartPole 환경의 자세한 사항은 https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py 에서 확인.\r\n",
    "        s = env.reset()\r\n",
    "        done = False\r\n",
    "\r\n",
    "        while not done:\r\n",
    "            # numpy.ndarray인 s를 tensor로 바꿔서 policy input으로 넣어준다. 그럼 output으로 확률분포를 얻는다. \r\n",
    "            # CartPole의 경우 action이 2개이다. 왼쪽으로 밀기, 오른쪽으로 밀기\r\n",
    "            # 예를 들어 (왼쪽으로 밀 확률 0.8, 오른쪽으로 밀 확률 0.2) 와 같이 확률 분포가 주어진다.\r\n",
    "            prob = pi(torch.from_numpy(s).float())\r\n",
    "            # policy가 stochastic policy이므로 sampling을 해야 한다.\r\n",
    "            # pytorch의 Categorical은 확률분포 모델이다.\r\n",
    "            m = Categorical(prob)\r\n",
    "            # 이 모델에서 sample을 호출하면 확률분포에 맞게 action을 tensor로 뽑아준다.\r\n",
    "            a = m.sample()\r\n",
    "            # env.step에 action을 주면 그 결과의 observation을 얻는다. =state transition\r\n",
    "            # a.item()은 tensor에서 scalar를 추출하기 위해 호출한 것이다.\r\n",
    "            s_prime, r, done, info = env.step(a.item())\r\n",
    "            # REINFORCE 알고리즘은 return이 필요하기 때문에 에피소드가 끝나야 학습할 수 있다.\r\n",
    "            # for 문을 돌면서 얻는 경험을 policy에 쌓아두기만 한다.\r\n",
    "            # (현재 reward, 현재 action을 선택할 확률)\r\n",
    "            pi.put_data((r, prob[a]))\r\n",
    "            s = s_prime\r\n",
    "            # score는 reward의 누적인데, reward는 매 스텝을 버틸 때마다 +1이 주어진다.\r\n",
    "            score += r\r\n",
    "\r\n",
    "        # 에피소드가 종료되었으므로 학습시킨다.\r\n",
    "        pi.train_net()\r\n",
    "\r\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\r\n",
    "            # 20 에피소드 평균 score를 출력함.\r\n",
    "            print(f\"# of epi: {n_epi}, avg score: {score/print_interval}\")\r\n",
    "            score = 0.0\r\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "if __name__ == '__main__':\r\n",
    "    main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# of epi: 20, avg score: 21.9\n",
      "# of epi: 40, avg score: 21.8\n",
      "# of epi: 60, avg score: 28.9\n",
      "# of epi: 80, avg score: 27.8\n",
      "# of epi: 100, avg score: 36.45\n",
      "# of epi: 120, avg score: 39.05\n",
      "# of epi: 140, avg score: 26.55\n",
      "# of epi: 160, avg score: 37.6\n",
      "# of epi: 180, avg score: 44.95\n",
      "# of epi: 200, avg score: 61.85\n",
      "# of epi: 220, avg score: 43.65\n",
      "# of epi: 240, avg score: 42.2\n",
      "# of epi: 260, avg score: 68.25\n",
      "# of epi: 280, avg score: 53.9\n",
      "# of epi: 300, avg score: 63.1\n",
      "# of epi: 320, avg score: 66.35\n",
      "# of epi: 340, avg score: 80.75\n",
      "# of epi: 360, avg score: 67.7\n",
      "# of epi: 380, avg score: 68.0\n",
      "# of epi: 400, avg score: 102.6\n",
      "# of epi: 420, avg score: 91.55\n",
      "# of epi: 440, avg score: 129.75\n",
      "# of epi: 460, avg score: 120.45\n",
      "# of epi: 480, avg score: 151.2\n",
      "# of epi: 500, avg score: 156.4\n",
      "# of epi: 520, avg score: 143.5\n",
      "# of epi: 540, avg score: 195.35\n",
      "# of epi: 560, avg score: 160.95\n",
      "# of epi: 580, avg score: 212.1\n",
      "# of epi: 600, avg score: 195.95\n",
      "# of epi: 620, avg score: 230.6\n",
      "# of epi: 640, avg score: 192.3\n",
      "# of epi: 660, avg score: 207.75\n",
      "# of epi: 680, avg score: 184.1\n",
      "# of epi: 700, avg score: 218.45\n",
      "# of epi: 720, avg score: 246.6\n",
      "# of epi: 740, avg score: 206.2\n",
      "# of epi: 760, avg score: 226.1\n",
      "# of epi: 780, avg score: 289.45\n",
      "# of epi: 800, avg score: 252.8\n",
      "# of epi: 820, avg score: 291.1\n",
      "# of epi: 840, avg score: 276.95\n",
      "# of epi: 860, avg score: 277.95\n",
      "# of epi: 880, avg score: 296.75\n",
      "# of epi: 900, avg score: 313.75\n",
      "# of epi: 920, avg score: 223.7\n",
      "# of epi: 940, avg score: 196.55\n",
      "# of epi: 960, avg score: 243.3\n",
      "# of epi: 980, avg score: 240.3\n",
      "# of epi: 1000, avg score: 252.95\n",
      "# of epi: 1020, avg score: 255.2\n",
      "# of epi: 1040, avg score: 311.05\n",
      "# of epi: 1060, avg score: 330.5\n",
      "# of epi: 1080, avg score: 383.1\n",
      "# of epi: 1100, avg score: 321.0\n",
      "# of epi: 1120, avg score: 247.45\n",
      "# of epi: 1140, avg score: 341.15\n",
      "# of epi: 1160, avg score: 358.5\n",
      "# of epi: 1180, avg score: 380.05\n",
      "# of epi: 1200, avg score: 306.85\n",
      "# of epi: 1220, avg score: 365.7\n",
      "# of epi: 1240, avg score: 206.95\n",
      "# of epi: 1260, avg score: 226.75\n",
      "# of epi: 1280, avg score: 245.55\n",
      "# of epi: 1300, avg score: 273.2\n",
      "# of epi: 1320, avg score: 302.65\n",
      "# of epi: 1340, avg score: 383.85\n",
      "# of epi: 1360, avg score: 390.7\n",
      "# of epi: 1380, avg score: 304.6\n",
      "# of epi: 1400, avg score: 288.1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-51ea7fc5d5bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-56de17c4802f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# CartPole의 경우 action이 2개이다. 왼쪽으로 밀기, 오른쪽으로 밀기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;31m# 예를 들어 (왼쪽으로 밀 확률 0.8, 오른쪽으로 밀 확률 0.2) 와 같이 확률 분포가 주어진다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;31m# policy가 stochastic policy이므로 sampling을 해야 한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m# pytorch의 Categorical은 확률분포 모델이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38-pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-9b10307ba9bc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# 4차원 feature vector를 128차원으로 변환하고 ReLU()를 취한 값을 tensor로 return한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# 128차원을 2차원으로 바꾸고 softmax()를 취한 값을 tensor로 return한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38-pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38-pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38-pytorch-gpu\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('py38-pytorch-gpu': conda)"
  },
  "interpreter": {
   "hash": "92bb61e8f2b0dcdaa21cd71aad1d97e3da046ad8a677ac321cbd25d595832889"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}