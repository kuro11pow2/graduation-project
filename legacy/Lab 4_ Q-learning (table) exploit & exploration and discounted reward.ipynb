{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 4: Q-learning (table) exploit & exploration and discounted reward.ipynb","provenance":[],"authorship_tag":"ABX9TyObpKXF0d4wFkYj1fxSGrhO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"7rIhm3T_A3fm","executionInfo":{"status":"ok","timestamp":1615128764005,"user_tz":-540,"elapsed":1007,"user":{"displayName":"JK Park","photoUrl":"","userId":"11909482312774240402"}}},"source":["# print('### OS ###')\r\n","# !cat /etc/issue.net\r\n","\r\n","# print('### CPU ###')\r\n","# !cat /proc/cpuinfo\r\n","\r\n","# print('### MEM ###')\r\n","# !cat /proc/meminfo\r\n","\r\n","# print('### DISK ###')\r\n","# !df -h\r\n","\r\n","# print('### GPU ###')\r\n","# !nvidia-smi"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DadeWo-NBD8S","executionInfo":{"status":"ok","timestamp":1615130069883,"user_tz":-540,"elapsed":4518,"user":{"displayName":"JK Park","photoUrl":"","userId":"11909482312774240402"}},"outputId":"e38c26bf-ff09-4106-a45f-c6c2b47cbe76"},"source":["import tensorflow as tf\r\n","print(tf.__version__)\r\n","\r\n","import torch\r\n","import torchvision\r\n","import torchtext\r\n","print(torch.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.4.1\n","1.7.1+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tj-mLW3Lw7n9","executionInfo":{"status":"ok","timestamp":1615130073898,"user_tz":-540,"elapsed":1489,"user":{"displayName":"JK Park","photoUrl":"","userId":"11909482312774240402"}},"outputId":"e337c7b7-e411-49c4-9a86-0882e40d663e"},"source":["import gym\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","from gym.envs.registration import register\r\n","\r\n","# 환경 설정. 미끄러지지 않고, 맵을 4 * 4로 한다.\r\n","register(\r\n","    id='FrozenLake-v3',\r\n","    entry_point='gym.envs.toy_text:FrozenLakeEnv',\r\n","    kwargs={'map_name': '4x4',\r\n","            'is_slippery': False}\r\n",")\r\n","\r\n","# 환경을 만든다.\r\n","env = gym.make(\"FrozenLake-v3\")\r\n","\r\n","# 환경의 상태를 출력한다\r\n","env.render() "],"execution_count":2,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g4NCTiU5OSDE"},"source":["# add random noise"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6eHza3klAlx1","executionInfo":{"status":"ok","timestamp":1615129910788,"user_tz":-540,"elapsed":2656,"user":{"displayName":"JK Park","photoUrl":"","userId":"11909482312774240402"}},"outputId":"b3d7a236-7cb7-4de4-a5a4-dcd3ea34233d"},"source":["state = env.reset()\r\n","env.render() \r\n","\r\n","# 상태와 액션의 쌍으로 된 공간을 만들고 0으로 초기화 = Q 함수를 0으로 초기화한다\r\n","Q = np.zeros([env.observation_space.n, env.action_space.n]) \r\n","\r\n","dis = .9  # discount factor\r\n","num_episodes = 10000  # 게임 수행하며 학습할 횟수\r\n","rList = []  # 게임 결과 저장. (reward의 총합)\r\n","\r\n","for i in range(num_episodes):\r\n","  # 새 게임이므로 상태 초기화한다.\r\n","  # Q 함수를 초기화하지 않는 것에 주목\r\n","  state = env.reset()\r\n","  rAll = 0\r\n","  done = False\r\n","\r\n","  # 게임 종료될 때까지 (goal을 찾을 때까지) 반복한다.\r\n","  while not done:\r\n","    # 현재 상태에서 어떤 행동을 할지 결정한다.\r\n","    # Q 함수의 값 + 랜덤 노이즈 + decaying이 적용되었다.\r\n","    action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1))\r\n","\r\n","    # 결정한 행동을 취한 결과에 대한 상태와 보상 등의 정보를 가져온다.\r\n","    new_state, reward, done, _ = env.step(action)\r\n","\r\n","    # 상태와 행동 쌍의 공간에 결과를 기록한다 = Q 함수를 갱신한다.\r\n","    # future reward에 dis가 곱해진 것을 확인하자.\r\n","    Q[state, action] = reward + dis * np.max(Q[new_state, :])\r\n","\r\n","    # goal 까지 가는 동안의 reward를 모두 더한다. (이 예제의 경우 찾으면 1 아니면 0이된다.)\r\n","    rAll += reward\r\n","    state = new_state\r\n","  \r\n","  rList.append(rAll)\r\n","\r\n","print('success rate: ' + str(sum(rList)/num_episodes))\r\n","print('Final Q-table values')\r\n","print('left down right up')\r\n","print(Q)\r\n","# plt.bar(range(len(rList)), rList, color='blue')\r\n","# plt.show()"],"execution_count":33,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","success rate: 0.9899\n","Final Q-table values\n","left down right up\n","[[0.      0.59049 0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.6561  0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.729   0.     ]\n"," [0.      0.81    0.      0.     ]\n"," [0.      0.9     0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.9     0.     ]\n"," [0.      0.      1.      0.     ]\n"," [0.      0.      0.      0.     ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QEDVemtBOJ3s"},"source":["# E-greedy\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IuYbuIIwOM0s","executionInfo":{"status":"ok","timestamp":1615130113510,"user_tz":-540,"elapsed":869,"user":{"displayName":"JK Park","photoUrl":"","userId":"11909482312774240402"}},"outputId":"7046d1ee-1aae-4ae7-8b3d-fc43c9b95bef"},"source":["state = env.reset()\r\n","env.render() \r\n","\r\n","# 상태와 액션의 쌍으로 된 공간을 만들고 0으로 초기화 = Q 함수를 0으로 초기화한다\r\n","Q = np.zeros([env.observation_space.n, env.action_space.n]) \r\n","\r\n","dis = .9  # discount factor\r\n","num_episodes = 2000  # 게임 수행하며 학습할 횟수\r\n","rList = []  # 게임 결과 저장. (reward의 총합)\r\n","\r\n","for i in range(num_episodes):\r\n","  # 새 게임이므로 상태 초기화한다.\r\n","  # Q 함수를 초기화하지 않는 것에 주목\r\n","  state = env.reset()\r\n","  rAll = 0\r\n","  done = False\r\n","  e = 1. / ((i / 100) + 1)\r\n","  # 게임 종료될 때까지 (goal을 찾을 때까지) 반복한다.\r\n","  while not done:\r\n","    # 현재 상태에서 어떤 행동을 할지 결정한다.\r\n","    # Q 함수의 값 + 랜덤 노이즈 + decaying이 적용되었다.\r\n","    if np.random.rand(1) < e:\r\n","      action = env.action_space.sample()\r\n","    else:\r\n","      action = np.argmax(Q[state, :])\r\n","\r\n","    # 결정한 행동을 취한 결과에 대한 상태와 보상 등의 정보를 가져온다.\r\n","    new_state, reward, done, _ = env.step(action)\r\n","\r\n","    # 상태와 행동 쌍의 공간에 결과를 기록한다 = Q 함수를 갱신한다.\r\n","    # future reward에 dis가 곱해진 것을 확인하자.\r\n","    Q[state, action] = reward + dis * np.max(Q[new_state, :])\r\n","\r\n","    # goal 까지 가는 동안의 reward를 모두 더한다. (이 예제의 경우 찾으면 1 아니면 0이된다.)\r\n","    rAll += reward\r\n","    state = new_state\r\n","  \r\n","  rList.append(rAll)\r\n","\r\n","print('success rate: ' + str(sum(rList)/num_episodes))\r\n","print('Final Q-table values')\r\n","print('left down right up')\r\n","print(Q)\r\n","# plt.bar(range(len(rList)), rList, color='blue')\r\n","# plt.show()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","success rate: 0.8075\n","Final Q-table values\n","left down right up\n","[[0.531441   0.59049    0.4782969  0.531441  ]\n"," [0.531441   0.         0.43046721 0.4782969 ]\n"," [0.4782969  0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.59049    0.6561     0.         0.531441  ]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.6561     0.         0.729      0.59049   ]\n"," [0.6561     0.81       0.81       0.        ]\n"," [0.729      0.9        0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.81       0.9        0.729     ]\n"," [0.81       0.9        1.         0.81      ]\n"," [0.         0.         0.         0.        ]]\n"],"name":"stdout"}]}]}