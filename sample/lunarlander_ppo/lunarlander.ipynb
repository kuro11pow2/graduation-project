{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd092bb61e8f2b0dcdaa21cd71aad1d97e3da046ad8a677ac321cbd25d595832889",
   "display_name": "Python 3.8.8 64-bit ('py38-pytorch-gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pybox2d\n",
    "# ref: https://github.com/CVxTz/RL\n",
    "\n",
    "# 시각화 \n",
    "# tensorboard --logdir ./logs\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import (\n",
    "    PolicyNetwork,\n",
    "    ValueNetwork,\n",
    "    device,\n",
    "    train_value_network,\n",
    "    train_policy_network,\n",
    ")\n",
    "from replay import Episode, History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    env_name=\"LunarLander-v2\",\n",
    "    reward_scale=20.0,\n",
    "    clip=0.2,\n",
    "    log_dir=\"../logs\",\n",
    "    learning_rate=0.001,\n",
    "    state_scale=1.0,\n",
    "):\n",
    "    writer = SummaryWriter(log_dir=log_dir, filename_suffix=env_name, comment=env_name)\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    observation = env.reset()\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    feature_dim = observation.size\n",
    "\n",
    "    value_model = ValueNetwork(in_dim=feature_dim).to(device)\n",
    "    value_optimizer = optim.Adam(value_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    policy_model = PolicyNetwork(in_dim=feature_dim, n=n_actions).to(device)\n",
    "    policy_optimizer = optim.Adam(policy_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    n_epoch = 4\n",
    "\n",
    "    max_episodes = 20\n",
    "    max_timesteps = 400\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    max_iterations = 200\n",
    "\n",
    "    history = History()\n",
    "\n",
    "    epoch_ite = 0\n",
    "    episode_ite = 0\n",
    "\n",
    "    for ite in tqdm(range(max_iterations)):\n",
    "\n",
    "        if ite % 50 == 0:\n",
    "            torch.save(\n",
    "                policy_model.state_dict(),\n",
    "                Path(log_dir) / (env_name + f\"_{str(ite)}_policy.pth\"),\n",
    "            )\n",
    "            torch.save(\n",
    "                value_model.state_dict(),\n",
    "                Path(log_dir) / (env_name + f\"_{str(ite)}_value.pth\"),\n",
    "            )\n",
    "\n",
    "        for episode_i in range(max_episodes):\n",
    "\n",
    "            observation = env.reset()\n",
    "            episode = Episode()\n",
    "\n",
    "            for timestep in range(max_timesteps):\n",
    "\n",
    "                action, log_probability = policy_model.sample_action(\n",
    "                    observation / state_scale\n",
    "                )\n",
    "                value = value_model.state_value(observation / state_scale)\n",
    "\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "\n",
    "                episode.append(\n",
    "                    observation=observation / state_scale,\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    value=value,\n",
    "                    log_probability=log_probability,\n",
    "                    reward_scale=reward_scale,\n",
    "                )\n",
    "\n",
    "                observation = new_observation\n",
    "\n",
    "                if done:\n",
    "                    episode.end_episode(last_value=0)\n",
    "                    break\n",
    "\n",
    "                if timestep == max_timesteps - 1:\n",
    "                    value = value_model.state_value(observation / state_scale)\n",
    "                    episode.end_episode(last_value=value)\n",
    "\n",
    "            episode_ite += 1\n",
    "            writer.add_scalar(\n",
    "                \"Average Episode Reward\",\n",
    "                reward_scale * np.sum(episode.rewards),\n",
    "                episode_ite,\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Average Probabilities\",\n",
    "                np.exp(np.mean(episode.log_probabilities)),\n",
    "                episode_ite,\n",
    "            )\n",
    "\n",
    "            history.add_episode(episode)\n",
    "\n",
    "        history.build_dataset()\n",
    "        data_loader = DataLoader(history, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        policy_loss = train_policy_network(\n",
    "            policy_model, policy_optimizer, data_loader, epochs=n_epoch, clip=clip\n",
    "        )\n",
    "\n",
    "        value_loss = train_value_network(\n",
    "            value_model, value_optimizer, data_loader, epochs=n_epoch\n",
    "        )\n",
    "\n",
    "        for p_l, v_l in zip(policy_loss, value_loss):\n",
    "            epoch_ite += 1\n",
    "            writer.add_scalar(\"Policy Loss\", p_l, epoch_ite)\n",
    "            writer.add_scalar(\"Value Loss\", v_l, epoch_ite)\n",
    "\n",
    "        history.free_memory()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 200/200 [1:05:18<00:00, 19.59s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\n",
    "        reward_scale=20.0,\n",
    "        clip=0.2,\n",
    "        env_name=\"LunarLander-v2\",\n",
    "        learning_rate=0.001,\n",
    "        state_scale=1.0,\n",
    "        log_dir=\"logs/Lunar\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}