{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "run anaconda prompt \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "home_path = os.path.expanduser('~')\n",
    "cur_path = os.getcwd()\n",
    "conda_path = home_path + \"\\\\anaconda3\"\n",
    "conda_script_path = home_path + \"\\\\anaconda3\\\\Scripts\\\\activate.bat\"\n",
    "exc = ' '.join(['start', '%windir%\\System32\\cmd.exe \"/K\"', conda_script_path, conda_path])\n",
    "!$exc\n",
    "\n",
    "\"\"\"\n",
    "run tensorboard server\n",
    "\n",
    "conda activate py38-pytorch-gpu && tensorboard --port=6006 --logdir=runs\n",
    "http://localhost:6006/\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "print env info\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "run tensorboard server\n",
    "\n",
    "conda activate py38-pytorch-gpu && tensorboard --port=6006 --logdir=runs\n",
    "http://localhost:6006/\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "print env info\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "from env import Env\n",
    "\n",
    "env = gym.make(Env.BIPEDALWALKER.value)\n",
    "\n",
    "print(dir(env))\n",
    "print(f'{env._max_episode_steps=}')\n",
    "print(f'{env.action_space=}')\n",
    "print(f'{env.metadata=}')\n",
    "print(f'{env.observation_space.shape[0]=}')\n",
    "print(f'{env.reward_range=}')\n",
    "print(f'{env.seed=}')\n",
    "print(f'{env.spec=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Env\n",
    "from runner import RunnerParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dqn_pole_train():\n",
    "    from algorithms.dqn import DQNParams\n",
    "    from algorithms.dqn_runner import DQNRunner\n",
    "    algo_param = DQNParams(buffer_limit=50000, n_train_start=4000,\n",
    "                            n_node=128, start_epsilon=0.1, learning_rate=0.0001,\n",
    "                            update_interval=40)\n",
    "    runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\n",
    "                                target_score=500.0,\n",
    "                                interval=8, \n",
    "                                max_video=100, video_record_interval=200,\n",
    "                                reward_scale=100.0)\n",
    "    DQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\n",
    "\n",
    "\n",
    "def dqn_pole_load():\n",
    "    from algorithms.dqn import DQNParams\n",
    "    from algorithms.dqn_runner import DQNRunner\n",
    "    algo_param = DQNParams(buffer_limit=50000, n_train_start=4000,\n",
    "                            n_node=128, start_epsilon=0.1, learning_rate=0.0001,\n",
    "                            update_interval=40)\n",
    "    runner_param = RunnerParams(train=False,\n",
    "                                load_net=True, \n",
    "                                load_name='DQN-CartPole-v1-500.0-train=True-intvl=8-rwdscl=100.0-node=128-lRate=0.0001-gma=0.98-nBuf=50000-nBat=32-nStrt=4000-updIntvl=40-1632225762.pt',\n",
    "                                name_postfix=str(algo_param),\n",
    "                                target_score=999.0,\n",
    "                                max_video=100, \n",
    "                                interval=1, video_record_interval=1,\n",
    "                                reward_scale=100.0)\n",
    "    DQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqn_pole_train():\n",
    "    from algorithms.ddqn import DDQNParams\n",
    "    from algorithms.ddqn_runner import DDQNRunner\n",
    "    algo_param = DDQNParams(buffer_limit=50000, n_train_start=2000,\n",
    "                            batch_size=32, gamma=0.98,\n",
    "                            n_node=128, start_epsilon=0.08, learning_rate=0.0005,\n",
    "                            update_interval=20)\n",
    "    runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\n",
    "                                target_score=500.0,\n",
    "                                interval=8, \n",
    "                                max_video=100, video_record_interval=200,\n",
    "                                reward_scale=100.0)\n",
    "    DDQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\n",
    "\n",
    "\n",
    "def ddqn_pole_load():\n",
    "    from algorithms.ddqn import DDQNParams\n",
    "    from algorithms.ddqn_runner import DDQNRunner\n",
    "    algo_param = DDQNParams(buffer_limit=50000, n_train_start=2000,\n",
    "                            batch_size=32, gamma=0.98,\n",
    "                            n_node=128, start_epsilon=0.08, learning_rate=0.0005,\n",
    "                            update_interval=20)\n",
    "    runner_param = RunnerParams(train=False,\n",
    "                                load_net=True, \n",
    "                                load_name='DDQN-CartPole-v1-500.0-train=True-intvl=8-rwdscl=100.0-node=128-lRate=0.0005-gma=0.98-nBuf=50000-nBat=32-nStrt=2000-updIntvl=20-1632228354.pt',\n",
    "                                name_postfix=str(algo_param),\n",
    "                                target_score=999.0,\n",
    "                                max_video=100, \n",
    "                                interval=1, video_record_interval=1,\n",
    "                                reward_scale=100.0)\n",
    "    DDQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ac_pole_train():\n",
    "    from algorithms.actorcritic import ActorCriticParams\n",
    "    from algorithms.actorcritic_runner import ActorCriticRunner\n",
    "    algo_param = ActorCriticParams(n_node=256, learning_rate=0.0002,\n",
    "                                    gamma=0.98, n_rollout=10)\n",
    "    runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\n",
    "                                target_score=500.0,\n",
    "                                interval=8, \n",
    "                                max_video=100, video_record_interval=200,\n",
    "                                reward_scale=100.0)\n",
    "    ActorCriticRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\n",
    "\n",
    "\n",
    "def ac_pole_load():\n",
    "    from algorithms.actorcritic import ActorCriticParams\n",
    "    from algorithms.actorcritic_runner import ActorCriticRunner\n",
    "    algo_param = ActorCriticParams(n_node=256, learning_rate=0.0002,\n",
    "                                    gamma=0.98, n_rollout=10)\n",
    "    runner_param = RunnerParams(train=False,\n",
    "                                load_net=True, \n",
    "                                load_name='ActorCritic-CartPole-v1-500.0-train=True-intvl=8-rwdscl=100.0-node=256-lRate=0.0002-gma=0.98-nRoll=10-1632236701.pt',\n",
    "                                name_postfix=str(algo_param),\n",
    "                                target_score=999.0,\n",
    "                                max_video=100, \n",
    "                                interval=1, video_record_interval=1,\n",
    "                                reward_scale=100.0)\n",
    "    ActorCriticRunner(Env.CARTPOLE.value, algo_param, runner_param).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "시뮬레이션 시작\n",
      "에피소드: 9, 평균 점수: 20.5\n",
      "에피소드: 19, 평균 점수: 22.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8444/3822481809.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mReinforceRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCARTPOLE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunner_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mrein_pole_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8444/3822481809.py\u001b[0m in \u001b[0;36mrein_pole_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                                 \u001b[0mmax_video\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_record_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                 reward_scale=100.0)\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mReinforceRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCARTPOLE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunner_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kuro1\\Source\\Repos\\Remote\\Univ\\graduation-project\\src\\runner.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf'-{(str(int(time.time())))}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kuro1\\Source\\Repos\\Remote\\Univ\\graduation-project\\src\\runner.py\u001b[0m in \u001b[0;36m_episode_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_epi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kuro1\\Source\\Repos\\Remote\\Univ\\graduation-project\\src\\algorithms\\reinforce_runner.py\u001b[0m in \u001b[0;36m_episode_sim\u001b[1;34m(self, n_epi)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_algo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\kuro1\\Source\\Repos\\Remote\\Univ\\graduation-project\\src\\algorithms\\reinfoce.py\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38-pytorch-gpu\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38-pytorch-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from env import Env\n",
    "from runner import RunnerParams\n",
    "\n",
    "def rein_pole_train():\n",
    "    from algorithms.reinfoce import ReinforceParams\n",
    "    from algorithms.reinforce_runner import ReinforceRunner\n",
    "    algo_param = ReinforceParams()\n",
    "    runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\n",
    "                                target_score=500.0,\n",
    "                                interval=10, \n",
    "                                max_video=100, video_record_interval=200,\n",
    "                                reward_scale=100.0)\n",
    "    ReinforceRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\n",
    "\n",
    "\n",
    "def rein_pole_load():\n",
    "    from algorithms.reinfoce import ReinforceParams\n",
    "    from algorithms.reinforce_runner import ReinforceRunner\n",
    "    algo_param = ReinforceParams()\n",
    "    runner_param = RunnerParams(train=False,\n",
    "                                load_net=True, \n",
    "                                load_name='Reinforce-CartPole-v1-500.0-train=True-intvl=10-rwdscl=100.0-node=128-lRate=0.0005-gma=0.98-1634489088.pt',\n",
    "                                name_postfix=str(algo_param),\n",
    "                                target_score=999.0,\n",
    "                                max_video=100, \n",
    "                                interval=1, video_record_interval=1,\n",
    "                                reward_scale=100.0)\n",
    "    ReinforceRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\n",
    "\n",
    "rein_pole_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_pole_train():\n",
    "    from algorithms.ppo import PPOParams\n",
    "    from algorithms.ppo_runner import PPORunner\n",
    "    algo_param = PPOParams(n_node=128, learning_rate=0.0001, \n",
    "                            gamma=0.98, lmbda=0.95, eps_clip=0.1, \n",
    "                            k_epoch=3, t_horizon=20)\n",
    "    runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\n",
    "                                target_score=500.0,\n",
    "                                interval=100, \n",
    "                                max_video=100, video_record_interval=200,\n",
    "                                reward_scale=100.0)\n",
    "    PPORunner(Env.CARTPOLE.value, algo_param, runner_param).run()\n",
    "\n",
    "\n",
    "def ppo_pole_load():\n",
    "    from algorithms.ppo import PPOParams\n",
    "    from algorithms.ppo_runner import PPORunner\n",
    "    algo_param = PPOParams(n_node=128, learning_rate=0.0001, \n",
    "                            gamma=0.98, lmbda=0.95, eps_clip=0.1, \n",
    "                            k_epoch=3, t_horizon=20)\n",
    "    runner_param = RunnerParams(train=False,\n",
    "                                load_net=True, \n",
    "                                load_name='PPO-CartPole-v1-500.0-train=True-intvl=100-rwdscl=100.0-node=128-lRate=0.0001-gma=0.98-lmb=0.95-epsclp=0.1-k=3-t=20-1632243563.pt',\n",
    "                                name_postfix=str(algo_param),\n",
    "                                target_score=999.0,\n",
    "                                max_video=100, \n",
    "                                interval=1, video_record_interval=1,\n",
    "                                reward_scale=100.0)\n",
    "    PPORunner(Env.CARTPOLE.value, algo_param, runner_param).run()\n",
    "\n",
    "\n",
    "def ppolstm_pole_train():\n",
    "    from algorithms.ppolstm import PPOlstmParams\n",
    "    from algorithms.ppolstm_runner import PPOlstmRunner\n",
    "    algo_param = PPOlstmParams(n_node=128, learning_rate=0.0001, \n",
    "                                gamma=0.98, lmbda=0.95, \n",
    "                                eps_clip=0.1, k_epoch=3, t_horizon=20)\n",
    "    runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\n",
    "                                target_score=500.0,\n",
    "                                interval=32, \n",
    "                                max_video=100, video_record_interval=200,\n",
    "                                reward_scale=100.0)\n",
    "    PPOlstmRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\n",
    "\n",
    "\n",
    "def ppolstm_pole_load():\n",
    "    from algorithms.ppolstm import PPOlstmParams\n",
    "    from algorithms.ppolstm_runner import PPOlstmRunner\n",
    "    algo_param = PPOlstmParams(n_node=128, learning_rate=0.0001, \n",
    "                                gamma=0.98, lmbda=0.95, \n",
    "                                eps_clip=0.1, k_epoch=3, t_horizon=20)\n",
    "    runner_param = RunnerParams(train=False,\n",
    "                                load_net=True, \n",
    "                                load_name='PPOlstm-CartPole-v1-500.0-train=True-intvl=32-rwdscl=100.0-node=128-lRate=0.0001-gma=0.98-lmb=0.95-epsclp=0.1-k=3-t=20-1632245913.pt',\n",
    "                                name_postfix=str(algo_param),\n",
    "                                target_score=999.0,\n",
    "                                max_video=100, \n",
    "                                interval=1, video_record_interval=1,\n",
    "                                reward_scale=100.0)\n",
    "    PPOlstmRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92bb61e8f2b0dcdaa21cd71aad1d97e3da046ad8a677ac321cbd25d595832889"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('py38-pytorch-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
