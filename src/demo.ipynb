{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "\r\n",
    "home_path = os.path.expanduser('~')\r\n",
    "cur_path = os.getcwd()\r\n",
    "conda_path = home_path + \"\\\\anaconda3\"\r\n",
    "conda_script_path = home_path + \"\\\\anaconda3\\\\Scripts\\\\activate.bat\"\r\n",
    "exc = ' '.join(['start', '%windir%\\System32\\cmd.exe \"/K\"', conda_script_path, conda_path])\r\n",
    "!$exc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conda activate py38-pytorch-gpu && tensorboard --port=6006 --logdir=runs\r\n",
    "http://localhost:6006/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gym\r\n",
    "from env import Env\r\n",
    "\r\n",
    "env = gym.make(Env.MOUNTAINCAR.value)\r\n",
    "print(dir(env))\r\n",
    "print(f'{env._max_episode_steps=}')\r\n",
    "print(f'{env.action_space=}')\r\n",
    "print(f'{env.metadata=}')\r\n",
    "print(f'{env.observation_space.shape[0]=}')\r\n",
    "print(f'{env.reward_range=}')\r\n",
    "print(f'{env.seed=}')\r\n",
    "print(f'{env.spec=}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "\r\n",
    "def dqn_pole_train():\r\n",
    "    from algorithms.dqn import DQNParams\r\n",
    "    from algorithms.dqn_runner import DQNRunner\r\n",
    "    algo_param = DQNParams(buffer_limit=50000, n_train_start=4000,\r\n",
    "                            n_node=128, start_epsilon=0.1, learning_rate=0.0001,\r\n",
    "                            update_interval=40)\r\n",
    "    runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\r\n",
    "                                target_score=500.0,\r\n",
    "                                interval=8, \r\n",
    "                                max_video=100, video_record_interval=200,\r\n",
    "                                reward_scale=100.0)\r\n",
    "    DQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\r\n",
    "\r\n",
    "\r\n",
    "def dqn_pole_load():\r\n",
    "    from algorithms.dqn import DQNParams\r\n",
    "    from algorithms.dqn_runner import DQNRunner\r\n",
    "    algo_param = DQNParams(buffer_limit=50000, n_train_start=4000,\r\n",
    "                            n_node=128, start_epsilon=0.1, learning_rate=0.0001,\r\n",
    "                            update_interval=40)\r\n",
    "    runner_param = RunnerParams(train=False,\r\n",
    "                                load_net=True, \r\n",
    "                                load_name='DQN-CartPole-v1-500.0-train=True-intvl=8-rwdscl=100.0-node=128-lRate=0.0001-gma=0.98-nBuf=50000-nBat=32-nStrt=4000-updIntvl=40-1632225762.pt',\r\n",
    "                                name_postfix=str(algo_param),\r\n",
    "                                target_score=999.0,\r\n",
    "                                max_video=100, \r\n",
    "                                interval=1, video_record_interval=1,\r\n",
    "                                reward_scale=100.0)\r\n",
    "    DQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\r\n",
    "\r\n",
    "\r\n",
    "def ddqn_pole_train():\r\n",
    "    from algorithms.ddqn import DDQNParams\r\n",
    "    from algorithms.ddqn_runner import DDQNRunner\r\n",
    "    algo_param = DDQNParams(buffer_limit=50000, n_train_start=2000,\r\n",
    "                            batch_size=32, gamma=0.98,\r\n",
    "                            n_node=128, start_epsilon=0.08, learning_rate=0.0005,\r\n",
    "                            update_interval=20)\r\n",
    "    runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\r\n",
    "                                target_score=500.0,\r\n",
    "                                interval=8, \r\n",
    "                                max_video=100, video_record_interval=200,\r\n",
    "                                reward_scale=100.0)\r\n",
    "    DDQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\r\n",
    "\r\n",
    "\r\n",
    "def ddqn_pole_load():\r\n",
    "    from algorithms.ddqn import DDQNParams\r\n",
    "    from algorithms.ddqn_runner import DDQNRunner\r\n",
    "    algo_param = DDQNParams(buffer_limit=50000, n_train_start=2000,\r\n",
    "                            batch_size=32, gamma=0.98,\r\n",
    "                            n_node=128, start_epsilon=0.08, learning_rate=0.0005,\r\n",
    "                            update_interval=20)\r\n",
    "    runner_param = RunnerParams(train=False,\r\n",
    "                                load_net=True, \r\n",
    "                                load_name='DDQN-CartPole-v1-500.0-train=True-intvl=8-rwdscl=100.0-node=128-lRate=0.0005-gma=0.98-nBuf=50000-nBat=32-nStrt=2000-updIntvl=20-1632228354.pt',\r\n",
    "                                name_postfix=str(algo_param),\r\n",
    "                                target_score=999.0,\r\n",
    "                                max_video=100, \r\n",
    "                                interval=1, video_record_interval=1,\r\n",
    "                                reward_scale=100.0)\r\n",
    "    DDQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\r\n",
    "\r\n",
    "ddqn_pole_load()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "시뮬레이션 시작\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kuro1\\anaconda3\\envs\\py38-pytorch-gpu\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] 스레드 모드가 설정된 후에는 바꿀 수 없습니다\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_epi=0, avg_score=122.0 비디오 저장\n",
      "에피소드: 0, 평균 점수: 122.0\n",
      "n_buffer : 122, eps : 8.0%\n",
      "에피소드: 8, 평균 점수: 72.0\n",
      "n_buffer : 954, eps : 8.0%\n",
      "에피소드: 16, 평균 점수: 91.0\n",
      "n_buffer : 1728, eps : 7.9%\n",
      "에피소드: 24, 평균 점수: 16.0\n",
      "n_buffer : 2112, eps : 7.9%\n",
      "에피소드: 32, 평균 점수: 15.0\n",
      "n_buffer : 2300, eps : 7.8%\n",
      "에피소드: 40, 평균 점수: 13.0\n",
      "n_buffer : 2492, eps : 7.8%\n",
      "에피소드: 48, 평균 점수: 87.0\n",
      "n_buffer : 2956, eps : 7.8%\n",
      "에피소드: 56, 평균 점수: 102.0\n",
      "n_buffer : 3282, eps : 7.7%\n",
      "에피소드: 64, 평균 점수: 35.0\n",
      "n_buffer : 3576, eps : 7.7%\n",
      "에피소드: 72, 평균 점수: 9.0\n",
      "n_buffer : 3760, eps : 7.6%\n",
      "에피소드: 80, 평균 점수: 9.0\n",
      "n_buffer : 4040, eps : 7.6%\n",
      "에피소드: 88, 평균 점수: 11.0\n",
      "n_buffer : 4185, eps : 7.6%\n",
      "에피소드: 96, 평균 점수: 9.0\n",
      "n_buffer : 4274, eps : 7.5%\n",
      "에피소드: 104, 평균 점수: 37.0\n",
      "n_buffer : 4484, eps : 7.5%\n",
      "에피소드: 112, 평균 점수: 54.0\n",
      "n_buffer : 5085, eps : 7.4%\n",
      "에피소드: 120, 평균 점수: 32.0\n",
      "n_buffer : 5430, eps : 7.4%\n",
      "에피소드: 128, 평균 점수: 88.0\n",
      "n_buffer : 6298, eps : 7.4%\n",
      "에피소드: 136, 평균 점수: 342.0\n",
      "n_buffer : 7884, eps : 7.3%\n",
      "에피소드: 144, 평균 점수: 41.0\n",
      "n_buffer : 8702, eps : 7.3%\n",
      "에피소드: 152, 평균 점수: 287.0\n",
      "n_buffer : 9931, eps : 7.2%\n",
      "에피소드: 160, 평균 점수: 140.0\n",
      "n_buffer : 11482, eps : 7.2%\n",
      "에피소드: 168, 평균 점수: 127.0\n",
      "n_buffer : 12444, eps : 7.2%\n",
      "에피소드: 176, 평균 점수: 26.0\n",
      "n_buffer : 13811, eps : 7.1%\n",
      "에피소드: 184, 평균 점수: 157.0\n",
      "n_buffer : 14680, eps : 7.1%\n",
      "에피소드: 192, 평균 점수: 217.0\n",
      "n_buffer : 16412, eps : 7.0%\n",
      "n_epi=200, avg_score=128.0 비디오 저장\n",
      "에피소드: 200, 평균 점수: 128.0\n",
      "n_buffer : 18149, eps : 7.0%\n",
      "에피소드: 208, 평균 점수: 219.0\n",
      "n_buffer : 19479, eps : 7.0%\n",
      "에피소드: 216, 평균 점수: 286.0\n",
      "n_buffer : 21393, eps : 6.9%\n",
      "에피소드: 224, 평균 점수: 46.0\n",
      "n_buffer : 22866, eps : 6.9%\n",
      "에피소드: 232, 평균 점수: 233.0\n",
      "n_buffer : 24253, eps : 6.8%\n",
      "에피소드: 240, 평균 점수: 145.0\n",
      "n_buffer : 25685, eps : 6.8%\n",
      "에피소드: 248, 평균 점수: 142.0\n",
      "n_buffer : 27190, eps : 6.8%\n",
      "에피소드: 256, 평균 점수: 235.0\n",
      "n_buffer : 28386, eps : 6.7%\n",
      "에피소드: 264, 평균 점수: 159.0\n",
      "n_buffer : 30407, eps : 6.7%\n",
      "에피소드: 272, 평균 점수: 245.0\n",
      "n_buffer : 31922, eps : 6.6%\n",
      "에피소드: 280, 평균 점수: 183.0\n",
      "n_buffer : 32850, eps : 6.6%\n",
      "에피소드: 288, 평균 점수: 66.0\n",
      "n_buffer : 34141, eps : 6.6%\n",
      "에피소드: 296, 평균 점수: 138.0\n",
      "n_buffer : 35868, eps : 6.5%\n",
      "에피소드: 304, 평균 점수: 137.0\n",
      "n_buffer : 37908, eps : 6.5%\n",
      "에피소드: 312, 평균 점수: 95.0\n",
      "n_buffer : 40257, eps : 6.4%\n",
      "에피소드: 320, 평균 점수: 201.0\n",
      "n_buffer : 42235, eps : 6.4%\n",
      "에피소드: 328, 평균 점수: 237.0\n",
      "n_buffer : 44281, eps : 6.4%\n",
      "에피소드: 336, 평균 점수: 134.0\n",
      "n_buffer : 45945, eps : 6.3%\n",
      "에피소드: 344, 평균 점수: 152.0\n",
      "n_buffer : 47524, eps : 6.3%\n",
      "에피소드: 352, 평균 점수: 198.0\n",
      "n_buffer : 48590, eps : 6.2%\n",
      "에피소드: 360, 평균 점수: 236.0\n",
      "n_buffer : 49505, eps : 6.2%\n",
      "에피소드: 368, 평균 점수: 130.0\n",
      "n_buffer : 50000, eps : 6.2%\n",
      "에피소드: 376, 평균 점수: 214.0\n",
      "n_buffer : 50000, eps : 6.1%\n",
      "에피소드: 384, 평균 점수: 155.0\n",
      "n_buffer : 50000, eps : 6.1%\n",
      "에피소드: 392, 평균 점수: 289.0\n",
      "n_buffer : 50000, eps : 6.0%\n",
      "n_epi=400, avg_score=131.0 비디오 저장\n",
      "에피소드: 400, 평균 점수: 131.0\n",
      "n_buffer : 50000, eps : 6.0%\n",
      "에피소드: 408, 평균 점수: 197.0\n",
      "n_buffer : 50000, eps : 6.0%\n",
      "에피소드: 416, 평균 점수: 162.0\n",
      "n_buffer : 50000, eps : 5.9%\n",
      "에피소드: 424, 평균 점수: 193.0\n",
      "n_buffer : 50000, eps : 5.9%\n",
      "에피소드: 432, 평균 점수: 58.0\n",
      "n_buffer : 50000, eps : 5.8%\n",
      "에피소드: 440, 평균 점수: 222.0\n",
      "n_buffer : 50000, eps : 5.8%\n",
      "에피소드: 448, 평균 점수: 151.0\n",
      "n_buffer : 50000, eps : 5.8%\n",
      "에피소드: 456, 평균 점수: 251.0\n",
      "n_buffer : 50000, eps : 5.7%\n",
      "에피소드: 464, 평균 점수: 156.0\n",
      "n_buffer : 50000, eps : 5.7%\n",
      "에피소드: 472, 평균 점수: 128.0\n",
      "n_buffer : 50000, eps : 5.6%\n",
      "에피소드: 480, 평균 점수: 119.0\n",
      "n_buffer : 50000, eps : 5.6%\n",
      "에피소드: 488, 평균 점수: 195.0\n",
      "n_buffer : 50000, eps : 5.6%\n",
      "에피소드: 496, 평균 점수: 249.0\n",
      "n_buffer : 50000, eps : 5.5%\n",
      "에피소드: 504, 평균 점수: 106.0\n",
      "n_buffer : 50000, eps : 5.5%\n",
      "에피소드: 512, 평균 점수: 126.0\n",
      "n_buffer : 50000, eps : 5.4%\n",
      "에피소드: 520, 평균 점수: 125.0\n",
      "n_buffer : 50000, eps : 5.4%\n",
      "에피소드: 528, 평균 점수: 168.0\n",
      "n_buffer : 50000, eps : 5.4%\n",
      "에피소드: 536, 평균 점수: 161.0\n",
      "n_buffer : 50000, eps : 5.3%\n",
      "에피소드: 544, 평균 점수: 175.0\n",
      "n_buffer : 50000, eps : 5.3%\n",
      "에피소드: 552, 평균 점수: 136.0\n",
      "n_buffer : 50000, eps : 5.2%\n",
      "에피소드: 560, 평균 점수: 147.0\n",
      "n_buffer : 50000, eps : 5.2%\n",
      "에피소드: 568, 평균 점수: 178.0\n",
      "n_buffer : 50000, eps : 5.2%\n",
      "에피소드: 576, 평균 점수: 152.0\n",
      "n_buffer : 50000, eps : 5.1%\n",
      "에피소드: 584, 평균 점수: 44.0\n",
      "n_buffer : 50000, eps : 5.1%\n",
      "에피소드: 592, 평균 점수: 148.0\n",
      "n_buffer : 50000, eps : 5.0%\n",
      "n_epi=600, avg_score=118.0 비디오 저장\n",
      "에피소드: 600, 평균 점수: 118.0\n",
      "n_buffer : 50000, eps : 5.0%\n",
      "에피소드: 608, 평균 점수: 123.0\n",
      "n_buffer : 50000, eps : 5.0%\n",
      "에피소드: 616, 평균 점수: 130.0\n",
      "n_buffer : 50000, eps : 4.9%\n",
      "에피소드: 624, 평균 점수: 140.0\n",
      "n_buffer : 50000, eps : 4.9%\n",
      "에피소드: 632, 평균 점수: 115.0\n",
      "n_buffer : 50000, eps : 4.8%\n",
      "에피소드: 640, 평균 점수: 20.0\n",
      "n_buffer : 50000, eps : 4.8%\n",
      "에피소드: 648, 평균 점수: 135.0\n",
      "n_buffer : 50000, eps : 4.8%\n",
      "에피소드: 656, 평균 점수: 188.0\n",
      "n_buffer : 50000, eps : 4.7%\n",
      "에피소드: 664, 평균 점수: 120.0\n",
      "n_buffer : 50000, eps : 4.7%\n",
      "에피소드: 672, 평균 점수: 108.0\n",
      "n_buffer : 50000, eps : 4.6%\n",
      "에피소드: 680, 평균 점수: 138.0\n",
      "n_buffer : 50000, eps : 4.6%\n",
      "에피소드: 688, 평균 점수: 145.0\n",
      "n_buffer : 50000, eps : 4.6%\n",
      "에피소드: 696, 평균 점수: 389.0\n",
      "n_buffer : 50000, eps : 4.5%\n",
      "에피소드: 704, 평균 점수: 169.0\n",
      "n_buffer : 50000, eps : 4.5%\n",
      "에피소드: 712, 평균 점수: 141.0\n",
      "n_buffer : 50000, eps : 4.4%\n",
      "에피소드: 720, 평균 점수: 161.0\n",
      "n_buffer : 50000, eps : 4.4%\n",
      "에피소드: 728, 평균 점수: 100.0\n",
      "n_buffer : 50000, eps : 4.4%\n",
      "에피소드: 736, 평균 점수: 105.0\n",
      "n_buffer : 50000, eps : 4.3%\n",
      "에피소드: 744, 평균 점수: 115.0\n",
      "n_buffer : 50000, eps : 4.3%\n",
      "에피소드: 752, 평균 점수: 207.0\n",
      "n_buffer : 50000, eps : 4.2%\n",
      "에피소드: 760, 평균 점수: 161.0\n",
      "n_buffer : 50000, eps : 4.2%\n",
      "에피소드: 768, 평균 점수: 202.0\n",
      "n_buffer : 50000, eps : 4.2%\n",
      "에피소드: 776, 평균 점수: 202.0\n",
      "n_buffer : 50000, eps : 4.1%\n",
      "에피소드: 784, 평균 점수: 192.0\n",
      "n_buffer : 50000, eps : 4.1%\n",
      "에피소드: 792, 평균 점수: 158.0\n",
      "n_buffer : 50000, eps : 4.0%\n",
      "n_epi=800, avg_score=136.0 비디오 저장\n",
      "에피소드: 800, 평균 점수: 136.0\n",
      "n_buffer : 50000, eps : 4.0%\n",
      "에피소드: 808, 평균 점수: 181.0\n",
      "n_buffer : 50000, eps : 4.0%\n",
      "에피소드: 816, 평균 점수: 54.0\n",
      "n_buffer : 50000, eps : 3.9%\n",
      "에피소드: 824, 평균 점수: 132.0\n",
      "n_buffer : 50000, eps : 3.9%\n",
      "에피소드: 832, 평균 점수: 29.0\n",
      "n_buffer : 50000, eps : 3.8%\n",
      "에피소드: 840, 평균 점수: 36.0\n",
      "n_buffer : 50000, eps : 3.8%\n",
      "에피소드: 848, 평균 점수: 114.0\n",
      "n_buffer : 50000, eps : 3.8%\n",
      "에피소드: 856, 평균 점수: 176.0\n",
      "n_buffer : 50000, eps : 3.7%\n",
      "에피소드: 864, 평균 점수: 209.0\n",
      "n_buffer : 50000, eps : 3.7%\n",
      "에피소드: 872, 평균 점수: 320.0\n",
      "n_buffer : 50000, eps : 3.6%\n",
      "에피소드: 880, 평균 점수: 482.0\n",
      "n_buffer : 50000, eps : 3.6%\n",
      "에피소드: 888, 평균 점수: 152.0\n",
      "n_buffer : 50000, eps : 3.6%\n",
      "에피소드: 896, 평균 점수: 100.0\n",
      "n_buffer : 50000, eps : 3.5%\n",
      "에피소드: 904, 평균 점수: 103.0\n",
      "n_buffer : 50000, eps : 3.5%\n",
      "에피소드: 912, 평균 점수: 196.0\n",
      "n_buffer : 50000, eps : 3.4%\n",
      "에피소드: 920, 평균 점수: 197.0\n",
      "n_buffer : 50000, eps : 3.4%\n",
      "에피소드: 928, 평균 점수: 132.0\n",
      "n_buffer : 50000, eps : 3.4%\n",
      "에피소드: 936, 평균 점수: 127.0\n",
      "n_buffer : 50000, eps : 3.3%\n",
      "에피소드: 944, 평균 점수: 61.0\n",
      "n_buffer : 50000, eps : 3.3%\n",
      "에피소드: 952, 평균 점수: 159.0\n",
      "n_buffer : 50000, eps : 3.2%\n",
      "에피소드: 960, 평균 점수: 134.0\n",
      "n_buffer : 50000, eps : 3.2%\n",
      "에피소드: 968, 평균 점수: 140.0\n",
      "n_buffer : 50000, eps : 3.2%\n",
      "에피소드: 976, 평균 점수: 38.0\n",
      "n_buffer : 50000, eps : 3.1%\n",
      "에피소드: 984, 평균 점수: 176.0\n",
      "n_buffer : 50000, eps : 3.1%\n",
      "에피소드: 992, 평균 점수: 291.0\n",
      "n_buffer : 50000, eps : 3.0%\n",
      "n_epi=1000, avg_score=167.0 비디오 저장\n",
      "에피소드: 1000, 평균 점수: 167.0\n",
      "n_buffer : 50000, eps : 3.0%\n",
      "에피소드: 1008, 평균 점수: 274.0\n",
      "n_buffer : 50000, eps : 3.0%\n",
      "에피소드: 1016, 평균 점수: 105.0\n",
      "n_buffer : 50000, eps : 2.9%\n",
      "에피소드: 1024, 평균 점수: 101.0\n",
      "n_buffer : 50000, eps : 2.9%\n",
      "에피소드: 1032, 평균 점수: 500.0\n",
      "n_buffer : 50000, eps : 2.8%\n",
      "종료 조건 만족. 최종 8번 평균 점수 500.0\n",
      "시뮬레이션 종료\n",
      "네트워크 저장\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cart-pole 모음\r\n",
    "\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "\r\n",
    "#dqn\r\n",
    "\r\n",
    "\r\n",
    "# ddqn\r\n",
    "# from algorithms.ddqn import DDQNParams\r\n",
    "# from algorithms.ddqn_runner import DDQNRunner\r\n",
    "# runner_param = RunnerParams(target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# DDQNRunner(Env.CARTPOLE.value, DDQNParams(), runner_param).run()\r\n",
    "\r\n",
    "# ActorCritic\r\n",
    "# from algorithms.actorcritic import ActorCriticParams\r\n",
    "# from algorithms.actorcritic_runner import ActorCriticRunner\r\n",
    "# runner_param = RunnerParams(target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# ActorCriticRunner(Env.CARTPOLE.value, ActorCriticParams(), runner_param).run()\r\n",
    "\r\n",
    "# PPO\r\n",
    "# from algorithms.ppo import PPOParams\r\n",
    "# from algorithms.ppo_runner import PPORunner\r\n",
    "\r\n",
    "# algo_param = PPOParams()\r\n",
    "\r\n",
    "# runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\r\n",
    "#                             target_score=500.0,\r\n",
    "#                             interval=40, \r\n",
    "#                             max_video=100, video_record_interval=200,\r\n",
    "#                             reward_scale=100.0)\r\n",
    "\r\n",
    "# PPORunner(Env.CARTPOLE.value, algo_param, runner_param).run()\r\n",
    "\r\n",
    "# PPOlstm\r\n",
    "# from algorithms.ppolstm import PPOlstmParams\r\n",
    "# from algorithms.ppolstm_runner import PPOlstmRunner\r\n",
    "# runner_param = RunnerParams(save_net=True, target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# PPOlstmRunner(Env.CARTPOLE.value, PPOlstmParams(), runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 모델 load\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "\r\n",
    "# dqn\r\n",
    "\r\n",
    "\r\n",
    "# ddqn\r\n",
    "# from algorithms.ddqn import DDQNParams\r\n",
    "# from algorithms.ddqn_runner import DDQNRunner\r\n",
    "# runner_param = RunnerParams(target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# DDQNRunner(Env.CARTPOLE.value, DDQNParams(), runner_param).run()\r\n",
    "\r\n",
    "# ActorCritic\r\n",
    "# from algorithms.actorcritic import ActorCriticParams\r\n",
    "# from algorithms.actorcritic_runner import ActorCriticRunner\r\n",
    "# runner_param = RunnerParams(target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# ActorCriticRunner(Env.CARTPOLE.value, ActorCriticParams(), runner_param).run()\r\n",
    "\r\n",
    "# PPO\r\n",
    "# from algorithms.ppo import PPOParams\r\n",
    "# from algorithms.ppo_runner import PPORunner\r\n",
    "\r\n",
    "# algo_param = PPOParams()\r\n",
    "# runner_param = RunnerParams(train=False, load_net=True, load_name='PPO-CartPole-v1-500.0-train=True-intvl=40-rwdscl=100.0-node=128-lRate=0.0001-gma=0.98-lmb=0.95-epsclp=0.1-k=3-t=20-1632072908.pt',\r\n",
    "#                             name_postfix=str(algo_param),\r\n",
    "#                             target_score=999.0,\r\n",
    "#                             max_video=100, \r\n",
    "#                             interval=1, video_record_interval=1,\r\n",
    "#                             reward_scale=100.0)\r\n",
    "\r\n",
    "# PPORunner(Env.CARTPOLE.value, algo_param, runner_param).run()\r\n",
    "\r\n",
    "# PPOlstm\r\n",
    "# from algorithms.ppolstm import PPOlstmParams\r\n",
    "# from algorithms.ppolstm_runner import PPOlstmRunner\r\n",
    "# runner_param = RunnerParams(save_net=True, target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# PPOlstmRunner(Env.CARTPOLE.value, PPOlstmParams(), runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## dqn cart-pole 학습\r\n",
    "\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "from algorithms.dqn import DQNParams\r\n",
    "from algorithms.dqn_runner import DQNRunner\r\n",
    "\r\n",
    "runner_param = RunnerParams(reward_scale=100.0, max_video=30)\r\n",
    "algo_param = DQNParams()\r\n",
    "DQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ddqn lunar-lander 불러오기\r\n",
    "\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "from algorithms.dqn import DQNParams\r\n",
    "from algorithms.dqn_runner import DQNRunner\r\n",
    "\r\n",
    "runner_param = RunnerParams(load_net=True, load_name='node512-score165-DQN-LunarLander-v2-1631737560.pt',\r\n",
    "                            train=False,\r\n",
    "                            target_score=150, \r\n",
    "                            reward_scale=30.0, max_video=100, video_record_interval=1,\r\n",
    "                            step_wrapper=lambda x: (x[0], x[1], x[2], x[3]))\r\n",
    "algo_param = DQNParams(n_node=512, batch_size=32, buffer_limit=100000, \r\n",
    "                        n_train_start=8000, start_epsilon=0.2,\r\n",
    "                        update_interval=40)\r\n",
    "DQNRunner(Env.LUNARLANDER.value, algo_param, runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ddqn lunar-lander\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "from algorithms.ddqn import DDQNParams\r\n",
    "from algorithms.ddqn_runner import DDQNRunner\r\n",
    "\r\n",
    "runner_param = RunnerParams(save_net=True,\r\n",
    "                            train=True,\r\n",
    "                            target_score=200, \r\n",
    "                            reward_scale=30.0, max_video=100, video_record_interval=200,\r\n",
    "                            step_wrapper=lambda x: x)\r\n",
    "algo_param = DDQNParams(n_node=512, batch_size=64, buffer_limit=50000, \r\n",
    "                        n_train_start=4000, start_epsilon=0.2,\r\n",
    "                        update_interval=20)\r\n",
    "DDQNRunner(Env.LUNARLANDER.value, algo_param, runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ddqn lunar-lander 불러오기\r\n",
    "\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "from algorithms.ddqn import DDQNParams\r\n",
    "from algorithms.ddqn_runner import DDQNRunner\r\n",
    "\r\n",
    "runner_param = RunnerParams(save_net=False, load_net=True, load_name='node512-score213-DDQN-LunarLander-v2-1631750946.pt',\r\n",
    "                            train=False,\r\n",
    "                            target_score=200, \r\n",
    "                            reward_scale=30.0, max_video=100, video_record_interval=1,\r\n",
    "                            step_wrapper=lambda x: x)\r\n",
    "algo_param = DDQNParams(n_node=512, batch_size=64, buffer_limit=50000, \r\n",
    "                        n_train_start=4000, start_epsilon=0.2,\r\n",
    "                        update_interval=20)\r\n",
    "DDQNRunner(Env.LUNARLANDER.value, algo_param, runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 알고리즘 테스트 \r\n",
    "\r\n",
    "# ppo\r\n",
    "\r\n",
    "# from tester import RunnerTester\r\n",
    "# from algorithms.ppo_runner import PPORunner\r\n",
    "# from algorithms.ppo import PPOParams\r\n",
    "# from env import Env\r\n",
    "# algo_params = PPOParams()\r\n",
    "# result = RunnerTester(PPORunner, algo_params, [Env.MOUNTAINCAR]).test()\r\n",
    "# print('통과' if result else '실패')\r\n",
    "\r\n",
    "\r\n",
    "# ppolstm\r\n",
    "\r\n",
    "# from tester import RunnerTester\r\n",
    "# from algorithms.ppolstm_runner import PPOlstmRunner\r\n",
    "# from algorithms.ppolstm import PPOlstmParams\r\n",
    "# from env import Env\r\n",
    "# algo_params = PPOlstmParams()\r\n",
    "# result = RunnerTester(PPOlstmRunner, algo_params, [Env.MOUNTAINCAR]).test()\r\n",
    "# print('통과' if result else '실패')\r\n",
    "\r\n",
    "\r\n",
    "# actorcritic\r\n",
    "\r\n",
    "# from tester import RunnerTester\r\n",
    "# from algorithms.actorcritic_runner import ActorCriticRunner\r\n",
    "# from algorithms.actorcritic import ActorCriticParams\r\n",
    "# from env import Env\r\n",
    "# algo_params = ActorCriticParams()\r\n",
    "# result = RunnerTester(ActorCriticRunner, algo_params, [Env.CARTPOLE]).test()\r\n",
    "# print('통과' if result else '실패')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# dqn\r\n",
    "# from tester import RunnerTester\r\n",
    "# from algorithms.dqn_runner import DQNRunner\r\n",
    "# from algorithms.dqn import DQNParams\r\n",
    "# from env import Env\r\n",
    "# algo_params = DQNParams()\r\n",
    "# result = RunnerTester(DQNRunner, algo_params, [Env.CARTPOLE]).test()\r\n",
    "# print('통과' if result else '실패')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# ddqn\r\n",
    "from tester import RunnerTester\r\n",
    "from algorithms.ddqn_runner import DDQNRunner\r\n",
    "from algorithms.ddqn import DDQNParams\r\n",
    "from env import Env\r\n",
    "algo_params = DDQNParams(start_epsilon=0.2, n_node=128, n_train_start=20000, \r\n",
    "                        buffer_limit=100000)\r\n",
    "result = RunnerTester(DDQNRunner, algo_params, [Env.CARTPOLE]).test()\r\n",
    "print('통과' if result else '실패')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('py38-pytorch-gpu': conda)"
  },
  "interpreter": {
   "hash": "92bb61e8f2b0dcdaa21cd71aad1d97e3da046ad8a677ac321cbd25d595832889"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}