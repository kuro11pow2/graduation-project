{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "\r\n",
    "home_path = os.path.expanduser('~')\r\n",
    "cur_path = os.getcwd()\r\n",
    "conda_path = home_path + \"\\\\anaconda3\"\r\n",
    "conda_script_path = home_path + \"\\\\anaconda3\\\\Scripts\\\\activate.bat\"\r\n",
    "exc = ' '.join(['start', '%windir%\\System32\\cmd.exe \"/K\"', conda_script_path, conda_path])\r\n",
    "!$exc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conda activate py38-pytorch-gpu && tensorboard --port=6006 --logdir=runs\r\n",
    "http://localhost:6006/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gym\r\n",
    "from env import Env\r\n",
    "\r\n",
    "env = gym.make(Env.MOUNTAINCAR.value)\r\n",
    "print(dir(env))\r\n",
    "print(f'{env._max_episode_steps=}')\r\n",
    "print(f'{env.action_space=}')\r\n",
    "print(f'{env.metadata=}')\r\n",
    "print(f'{env.observation_space.shape[0]=}')\r\n",
    "print(f'{env.reward_range=}')\r\n",
    "print(f'{env.seed=}')\r\n",
    "print(f'{env.spec=}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# cart-pole 모음\r\n",
    "\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "\r\n",
    "#dqn\r\n",
    "from algorithms.dqn import DQNParams\r\n",
    "from algorithms.dqn_runner import DQNRunner\r\n",
    "algo_param = DQNParams(buffer_limit=50000, n_train_start=4000,\r\n",
    "                        n_node=128, start_epsilon=0.1, learning_rate=0.0001,\r\n",
    "                        update_interval=40)\r\n",
    "\r\n",
    "runner_param = RunnerParams(save_net=True, name_postfix=str(algo_param),\r\n",
    "                            target_score=500.0,\r\n",
    "                            interval=1, \r\n",
    "                            max_video=100, video_record_interval=200,\r\n",
    "                            reward_scale=100.0)\r\n",
    "DQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()\r\n",
    "\r\n",
    "# ddqn\r\n",
    "# from algorithms.ddqn import DDQNParams\r\n",
    "# from algorithms.ddqn_runner import DDQNRunner\r\n",
    "# runner_param = RunnerParams(target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# DDQNRunner(Env.CARTPOLE.value, DDQNParams(), runner_param).run()\r\n",
    "\r\n",
    "# REINFORCE\r\n",
    "# from algorithms.actorcritic import ActorCriticParams\r\n",
    "# from algorithms.actorcritic_runner import ActorCriticRunner\r\n",
    "# runner_param = RunnerParams(target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# ActorCriticRunner(Env.CARTPOLE.value, ActorCriticParams(), runner_param).run()\r\n",
    "\r\n",
    "# PPO\r\n",
    "# from algorithms.ppo import PPOParams\r\n",
    "# from algorithms.ppo_runner import PPORunner\r\n",
    "# runner_param = RunnerParams(save_net=True, \r\n",
    "#                             target_score=500, \r\n",
    "#                             video_record_interval=200,\r\n",
    "#                             reward_scale=100.0, max_video=100)\r\n",
    "# PPORunner(Env.CARTPOLE.value, PPOParams(n_node=256), runner_param).run()\r\n",
    "\r\n",
    "# PPOlstm\r\n",
    "# from algorithms.ppolstm import PPOlstmParams\r\n",
    "# from algorithms.ppolstm_runner import PPOlstmRunner\r\n",
    "# runner_param = RunnerParams(save_net=True, target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# PPOlstmRunner(Env.CARTPOLE.value, PPOlstmParams(), runner_param).run()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "시뮬레이션 시작\n",
      "n_epi=0, avg_score=10.0 비디오 저장\n",
      "에피소드: 0, 평균 점수: 10.0\n",
      "n_buffer : 10, eps : 10.0%\n",
      "에피소드: 2, 평균 점수: 10.0\n",
      "n_buffer : 30, eps : 10.0%\n",
      "에피소드: 4, 평균 점수: 9.0\n",
      "n_buffer : 48, eps : 10.0%\n",
      "에피소드: 6, 평균 점수: 12.0\n",
      "n_buffer : 70, eps : 10.0%\n",
      "에피소드: 8, 평균 점수: 11.0\n",
      "n_buffer : 90, eps : 10.0%\n",
      "에피소드: 10, 평균 점수: 11.0\n",
      "n_buffer : 111, eps : 10.0%\n",
      "에피소드: 12, 평균 점수: 12.0\n",
      "n_buffer : 131, eps : 9.9%\n",
      "에피소드: 14, 평균 점수: 10.0\n",
      "n_buffer : 152, eps : 9.9%\n",
      "에피소드: 16, 평균 점수: 10.0\n",
      "n_buffer : 174, eps : 9.9%\n",
      "에피소드: 18, 평균 점수: 8.0\n",
      "n_buffer : 192, eps : 9.9%\n",
      "에피소드: 20, 평균 점수: 10.0\n",
      "n_buffer : 212, eps : 9.9%\n",
      "에피소드: 22, 평균 점수: 11.0\n",
      "n_buffer : 234, eps : 9.9%\n",
      "에피소드: 24, 평균 점수: 10.0\n",
      "n_buffer : 254, eps : 9.9%\n",
      "에피소드: 26, 평균 점수: 8.0\n",
      "n_buffer : 272, eps : 9.9%\n",
      "에피소드: 28, 평균 점수: 10.0\n",
      "n_buffer : 291, eps : 9.9%\n",
      "에피소드: 30, 평균 점수: 10.0\n",
      "n_buffer : 312, eps : 9.8%\n",
      "에피소드: 32, 평균 점수: 9.0\n",
      "n_buffer : 331, eps : 9.8%\n",
      "에피소드: 34, 평균 점수: 10.0\n",
      "n_buffer : 351, eps : 9.8%\n",
      "에피소드: 36, 평균 점수: 12.0\n",
      "n_buffer : 375, eps : 9.8%\n",
      "에피소드: 38, 평균 점수: 8.0\n",
      "n_buffer : 393, eps : 9.8%\n",
      "에피소드: 40, 평균 점수: 12.0\n",
      "n_buffer : 416, eps : 9.8%\n",
      "에피소드: 42, 평균 점수: 10.0\n",
      "n_buffer : 436, eps : 9.8%\n",
      "에피소드: 44, 평균 점수: 9.0\n",
      "n_buffer : 456, eps : 9.8%\n",
      "에피소드: 46, 평균 점수: 11.0\n",
      "n_buffer : 476, eps : 9.8%\n",
      "에피소드: 48, 평균 점수: 12.0\n",
      "n_buffer : 499, eps : 9.8%\n",
      "에피소드: 50, 평균 점수: 10.0\n",
      "n_buffer : 519, eps : 9.8%\n",
      "에피소드: 52, 평균 점수: 10.0\n",
      "n_buffer : 539, eps : 9.7%\n",
      "에피소드: 54, 평균 점수: 10.0\n",
      "n_buffer : 557, eps : 9.7%\n",
      "에피소드: 56, 평균 점수: 9.0\n",
      "n_buffer : 580, eps : 9.7%\n",
      "에피소드: 58, 평균 점수: 8.0\n",
      "n_buffer : 599, eps : 9.7%\n",
      "에피소드: 60, 평균 점수: 10.0\n",
      "n_buffer : 619, eps : 9.7%\n",
      "에피소드: 62, 평균 점수: 11.0\n",
      "n_buffer : 639, eps : 9.7%\n",
      "에피소드: 64, 평균 점수: 10.0\n",
      "n_buffer : 658, eps : 9.7%\n",
      "에피소드: 66, 평균 점수: 11.0\n",
      "n_buffer : 678, eps : 9.7%\n",
      "에피소드: 68, 평균 점수: 10.0\n",
      "n_buffer : 698, eps : 9.7%\n",
      "에피소드: 70, 평균 점수: 8.0\n",
      "n_buffer : 716, eps : 9.7%\n",
      "에피소드: 72, 평균 점수: 9.0\n",
      "n_buffer : 735, eps : 9.6%\n",
      "에피소드: 74, 평균 점수: 9.0\n",
      "n_buffer : 758, eps : 9.6%\n",
      "에피소드: 76, 평균 점수: 10.0\n",
      "n_buffer : 777, eps : 9.6%\n",
      "에피소드: 78, 평균 점수: 8.0\n",
      "n_buffer : 794, eps : 9.6%\n",
      "에피소드: 80, 평균 점수: 10.0\n",
      "n_buffer : 815, eps : 9.6%\n",
      "에피소드: 82, 평균 점수: 9.0\n",
      "n_buffer : 833, eps : 9.6%\n",
      "에피소드: 84, 평균 점수: 9.0\n",
      "n_buffer : 851, eps : 9.6%\n",
      "에피소드: 86, 평균 점수: 12.0\n",
      "n_buffer : 872, eps : 9.6%\n",
      "에피소드: 88, 평균 점수: 10.0\n",
      "n_buffer : 892, eps : 9.6%\n",
      "에피소드: 90, 평균 점수: 10.0\n",
      "n_buffer : 912, eps : 9.6%\n",
      "에피소드: 92, 평균 점수: 9.0\n",
      "n_buffer : 931, eps : 9.5%\n",
      "에피소드: 94, 평균 점수: 8.0\n",
      "n_buffer : 949, eps : 9.5%\n",
      "에피소드: 96, 평균 점수: 10.0\n",
      "n_buffer : 971, eps : 9.5%\n",
      "에피소드: 98, 평균 점수: 11.0\n",
      "n_buffer : 995, eps : 9.5%\n",
      "에피소드: 100, 평균 점수: 10.0\n",
      "n_buffer : 1014, eps : 9.5%\n",
      "에피소드: 102, 평균 점수: 10.0\n",
      "n_buffer : 1034, eps : 9.5%\n",
      "에피소드: 104, 평균 점수: 9.0\n",
      "n_buffer : 1053, eps : 9.5%\n",
      "에피소드: 106, 평균 점수: 11.0\n",
      "n_buffer : 1074, eps : 9.5%\n",
      "에피소드: 108, 평균 점수: 10.0\n",
      "n_buffer : 1096, eps : 9.5%\n",
      "에피소드: 110, 평균 점수: 9.0\n",
      "n_buffer : 1114, eps : 9.4%\n",
      "에피소드: 112, 평균 점수: 10.0\n",
      "n_buffer : 1133, eps : 9.4%\n",
      "에피소드: 114, 평균 점수: 9.0\n",
      "n_buffer : 1154, eps : 9.4%\n",
      "에피소드: 116, 평균 점수: 10.0\n",
      "n_buffer : 1174, eps : 9.4%\n",
      "에피소드: 118, 평균 점수: 9.0\n",
      "n_buffer : 1191, eps : 9.4%\n",
      "에피소드: 120, 평균 점수: 11.0\n",
      "n_buffer : 1213, eps : 9.4%\n",
      "에피소드: 122, 평균 점수: 10.0\n",
      "n_buffer : 1232, eps : 9.4%\n",
      "에피소드: 124, 평균 점수: 10.0\n",
      "n_buffer : 1254, eps : 9.4%\n",
      "에피소드: 126, 평균 점수: 11.0\n",
      "n_buffer : 1274, eps : 9.4%\n",
      "에피소드: 128, 평균 점수: 10.0\n",
      "n_buffer : 1294, eps : 9.4%\n",
      "에피소드: 130, 평균 점수: 9.0\n",
      "n_buffer : 1312, eps : 9.3%\n",
      "에피소드: 132, 평균 점수: 10.0\n",
      "n_buffer : 1332, eps : 9.3%\n",
      "에피소드: 134, 평균 점수: 14.0\n",
      "n_buffer : 1355, eps : 9.3%\n",
      "에피소드: 136, 평균 점수: 10.0\n",
      "n_buffer : 1374, eps : 9.3%\n",
      "에피소드: 138, 평균 점수: 8.0\n",
      "n_buffer : 1391, eps : 9.3%\n",
      "에피소드: 140, 평균 점수: 9.0\n",
      "n_buffer : 1410, eps : 9.3%\n",
      "에피소드: 142, 평균 점수: 9.0\n",
      "n_buffer : 1429, eps : 9.3%\n",
      "에피소드: 144, 평균 점수: 9.0\n",
      "n_buffer : 1446, eps : 9.3%\n",
      "에피소드: 146, 평균 점수: 12.0\n",
      "n_buffer : 1469, eps : 9.3%\n",
      "에피소드: 148, 평균 점수: 10.0\n",
      "n_buffer : 1488, eps : 9.3%\n",
      "에피소드: 150, 평균 점수: 10.0\n",
      "n_buffer : 1509, eps : 9.2%\n",
      "에피소드: 152, 평균 점수: 9.0\n",
      "n_buffer : 1528, eps : 9.2%\n",
      "에피소드: 154, 평균 점수: 12.0\n",
      "n_buffer : 1551, eps : 9.2%\n",
      "에피소드: 156, 평균 점수: 9.0\n",
      "n_buffer : 1574, eps : 9.2%\n",
      "에피소드: 158, 평균 점수: 8.0\n",
      "n_buffer : 1591, eps : 9.2%\n",
      "에피소드: 160, 평균 점수: 10.0\n",
      "n_buffer : 1609, eps : 9.2%\n",
      "에피소드: 162, 평균 점수: 11.0\n",
      "n_buffer : 1629, eps : 9.2%\n",
      "에피소드: 164, 평균 점수: 10.0\n",
      "n_buffer : 1648, eps : 9.2%\n",
      "에피소드: 166, 평균 점수: 8.0\n",
      "n_buffer : 1664, eps : 9.2%\n",
      "에피소드: 168, 평균 점수: 10.0\n",
      "n_buffer : 1682, eps : 9.2%\n",
      "에피소드: 170, 평균 점수: 9.0\n",
      "n_buffer : 1702, eps : 9.2%\n",
      "에피소드: 172, 평균 점수: 10.0\n",
      "n_buffer : 1720, eps : 9.1%\n",
      "에피소드: 174, 평균 점수: 11.0\n",
      "n_buffer : 1740, eps : 9.1%\n",
      "에피소드: 176, 평균 점수: 9.0\n",
      "n_buffer : 1759, eps : 9.1%\n",
      "에피소드: 178, 평균 점수: 9.0\n",
      "n_buffer : 1778, eps : 9.1%\n",
      "에피소드: 180, 평균 점수: 10.0\n",
      "n_buffer : 1797, eps : 9.1%\n",
      "에피소드: 182, 평균 점수: 9.0\n",
      "n_buffer : 1816, eps : 9.1%\n",
      "에피소드: 184, 평균 점수: 9.0\n",
      "n_buffer : 1834, eps : 9.1%\n",
      "에피소드: 186, 평균 점수: 9.0\n",
      "n_buffer : 1852, eps : 9.1%\n",
      "에피소드: 188, 평균 점수: 10.0\n",
      "n_buffer : 1871, eps : 9.1%\n",
      "에피소드: 190, 평균 점수: 10.0\n",
      "n_buffer : 1891, eps : 9.1%\n",
      "에피소드: 192, 평균 점수: 9.0\n",
      "n_buffer : 1910, eps : 9.0%\n",
      "에피소드: 194, 평균 점수: 10.0\n",
      "n_buffer : 1929, eps : 9.0%\n",
      "에피소드: 196, 평균 점수: 9.0\n",
      "n_buffer : 1948, eps : 9.0%\n",
      "에피소드: 198, 평균 점수: 10.0\n",
      "n_buffer : 1968, eps : 9.0%\n",
      "n_epi=200, avg_score=10.0 비디오 저장\n",
      "에피소드: 200, 평균 점수: 10.0\n",
      "n_buffer : 1988, eps : 9.0%\n",
      "종료 조건 만족. 최종 2번 평균 점수 10.0\n",
      "시뮬레이션 종료\n",
      "네트워크 저장\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 모델 load\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "\r\n",
    "# dqn\r\n",
    "# from algorithms.dqn import DQNParams\r\n",
    "# from algorithms.dqn_runner import DQNRunner\r\n",
    "# runner_param = RunnerParams(train=False, load_net=True, load_name='DQN-CartPole-v1-1631723356.pt',\r\n",
    "#                             target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# DQNRunner(Env.CARTPOLE.value, DQNParams(), runner_param).run()\r\n",
    "\r\n",
    "# ddqn\r\n",
    "# from algorithms.ddqn import DDQNParams\r\n",
    "# from algorithms.ddqn_runner import DDQNRunner\r\n",
    "# runner_param = RunnerParams(target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# DDQNRunner(Env.CARTPOLE.value, DDQNParams(), runner_param).run()\r\n",
    "\r\n",
    "# REINFORCE\r\n",
    "# from algorithms.actorcritic import ActorCriticParams\r\n",
    "# from algorithms.actorcritic_runner import ActorCriticRunner\r\n",
    "# runner_param = RunnerParams(target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# ActorCriticRunner(Env.CARTPOLE.value, ActorCriticParams(), runner_param).run()\r\n",
    "\r\n",
    "# PPO\r\n",
    "from algorithms.ppo import PPOParams\r\n",
    "from algorithms.ppo_runner import PPORunner\r\n",
    "runner_param = RunnerParams(train=False, load_net=True, load_name='PPO-CartPole-v1-1631783669.pt',\r\n",
    "                            target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "PPORunner(Env.CARTPOLE.value, PPOParams(n_node=256), runner_param).run()\r\n",
    "\r\n",
    "# PPOlstm\r\n",
    "# from algorithms.ppolstm import PPOlstmParams\r\n",
    "# from algorithms.ppolstm_runner import PPOlstmRunner\r\n",
    "# runner_param = RunnerParams(save_net=True, target_score=500, reward_scale=100.0, max_video=100)\r\n",
    "# PPOlstmRunner(Env.CARTPOLE.value, PPOlstmParams(), runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## dqn cart-pole 학습\r\n",
    "\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "from algorithms.dqn import DQNParams\r\n",
    "from algorithms.dqn_runner import DQNRunner\r\n",
    "\r\n",
    "runner_param = RunnerParams(reward_scale=100.0, max_video=30)\r\n",
    "algo_param = DQNParams()\r\n",
    "DQNRunner(Env.CARTPOLE.value, algo_param, runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ddqn lunar-lander 불러오기\r\n",
    "\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "from algorithms.dqn import DQNParams\r\n",
    "from algorithms.dqn_runner import DQNRunner\r\n",
    "\r\n",
    "runner_param = RunnerParams(load_net=True, load_name='node512-score165-DQN-LunarLander-v2-1631737560.pt',\r\n",
    "                            train=False,\r\n",
    "                            target_score=150, \r\n",
    "                            reward_scale=30.0, max_video=100, video_record_interval=1,\r\n",
    "                            step_wrapper=lambda x: (x[0], x[1], x[2], x[3]))\r\n",
    "algo_param = DQNParams(n_node=512, batch_size=32, buffer_limit=100000, \r\n",
    "                        n_train_start=8000, start_epsilon=0.2,\r\n",
    "                        update_interval=40)\r\n",
    "DQNRunner(Env.LUNARLANDER.value, algo_param, runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ddqn lunar-lander\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "from algorithms.ddqn import DDQNParams\r\n",
    "from algorithms.ddqn_runner import DDQNRunner\r\n",
    "\r\n",
    "runner_param = RunnerParams(save_net=True,\r\n",
    "                            train=True,\r\n",
    "                            target_score=200, \r\n",
    "                            reward_scale=30.0, max_video=100, video_record_interval=200,\r\n",
    "                            step_wrapper=lambda x: x)\r\n",
    "algo_param = DDQNParams(n_node=512, batch_size=64, buffer_limit=50000, \r\n",
    "                        n_train_start=4000, start_epsilon=0.2,\r\n",
    "                        update_interval=20)\r\n",
    "DDQNRunner(Env.LUNARLANDER.value, algo_param, runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ddqn lunar-lander 불러오기\r\n",
    "\r\n",
    "from env import Env\r\n",
    "from runner import RunnerParams\r\n",
    "from algorithms.ddqn import DDQNParams\r\n",
    "from algorithms.ddqn_runner import DDQNRunner\r\n",
    "\r\n",
    "runner_param = RunnerParams(save_net=False, load_net=True, load_name='node512-score213-DDQN-LunarLander-v2-1631750946.pt',\r\n",
    "                            train=False,\r\n",
    "                            target_score=200, \r\n",
    "                            reward_scale=30.0, max_video=100, video_record_interval=1,\r\n",
    "                            step_wrapper=lambda x: x)\r\n",
    "algo_param = DDQNParams(n_node=512, batch_size=64, buffer_limit=50000, \r\n",
    "                        n_train_start=4000, start_epsilon=0.2,\r\n",
    "                        update_interval=20)\r\n",
    "DDQNRunner(Env.LUNARLANDER.value, algo_param, runner_param).run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 알고리즘 테스트 \r\n",
    "\r\n",
    "# ppo\r\n",
    "\r\n",
    "# from tester import RunnerTester\r\n",
    "# from algorithms.ppo_runner import PPORunner\r\n",
    "# from algorithms.ppo import PPOParams\r\n",
    "# from env import Env\r\n",
    "# algo_params = PPOParams()\r\n",
    "# result = RunnerTester(PPORunner, algo_params, [Env.MOUNTAINCAR]).test()\r\n",
    "# print('통과' if result else '실패')\r\n",
    "\r\n",
    "\r\n",
    "# ppolstm\r\n",
    "\r\n",
    "# from tester import RunnerTester\r\n",
    "# from algorithms.ppolstm_runner import PPOlstmRunner\r\n",
    "# from algorithms.ppolstm import PPOlstmParams\r\n",
    "# from env import Env\r\n",
    "# algo_params = PPOlstmParams()\r\n",
    "# result = RunnerTester(PPOlstmRunner, algo_params, [Env.MOUNTAINCAR]).test()\r\n",
    "# print('통과' if result else '실패')\r\n",
    "\r\n",
    "\r\n",
    "# actorcritic\r\n",
    "\r\n",
    "# from tester import RunnerTester\r\n",
    "# from algorithms.actorcritic_runner import ActorCriticRunner\r\n",
    "# from algorithms.actorcritic import ActorCriticParams\r\n",
    "# from env import Env\r\n",
    "# algo_params = ActorCriticParams()\r\n",
    "# result = RunnerTester(ActorCriticRunner, algo_params, [Env.CARTPOLE]).test()\r\n",
    "# print('통과' if result else '실패')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# dqn\r\n",
    "# from tester import RunnerTester\r\n",
    "# from algorithms.dqn_runner import DQNRunner\r\n",
    "# from algorithms.dqn import DQNParams\r\n",
    "# from env import Env\r\n",
    "# algo_params = DQNParams()\r\n",
    "# result = RunnerTester(DQNRunner, algo_params, [Env.CARTPOLE]).test()\r\n",
    "# print('통과' if result else '실패')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# ddqn\r\n",
    "from tester import RunnerTester\r\n",
    "from algorithms.ddqn_runner import DDQNRunner\r\n",
    "from algorithms.ddqn import DDQNParams\r\n",
    "from env import Env\r\n",
    "algo_params = DDQNParams(start_epsilon=0.2, n_node=128, n_train_start=20000, \r\n",
    "                        buffer_limit=100000)\r\n",
    "result = RunnerTester(DDQNRunner, algo_params, [Env.CARTPOLE]).test()\r\n",
    "print('통과' if result else '실패')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('py38-pytorch-gpu': conda)"
  },
  "interpreter": {
   "hash": "92bb61e8f2b0dcdaa21cd71aad1d97e3da046ad8a677ac321cbd25d595832889"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}