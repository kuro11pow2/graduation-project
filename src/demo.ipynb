{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrun tensorboard server\\n\\nconda activate py38-pytorch-gpu && tensorboard --port=6006 --logdir=runs\\nhttp://localhost:6006/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "run anaconda prompt \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "home_path = os.path.expanduser('~')\n",
    "cur_path = os.getcwd()\n",
    "conda_path = home_path + \"\\\\anaconda3\"\n",
    "conda_script_path = home_path + \"\\\\anaconda3\\\\Scripts\\\\activate.bat\"\n",
    "exc = ' '.join(['start', '%windir%\\System32\\cmd.exe \"/K\"', conda_script_path, conda_path])\n",
    "!$exc\n",
    "\n",
    "\"\"\"\n",
    "run tensorboard server\n",
    "\n",
    "conda activate py38-pytorch-gpu && tensorboard --port=6006 --logdir=runs\n",
    "http://localhost:6006/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Env\n",
    "from runner import RunnerParams\n",
    "from logger import Logger\n",
    "from remover import Remover\n",
    "from algorithms.dqn import DQN, DQNParams\n",
    "from algorithms.dqn_runner import DQNRunner\n",
    "from algorithms.ddqn import DDQN, DDQNParams\n",
    "from algorithms.ddqn_runner import DDQNRunner\n",
    "from algorithms.reinforce import Reinforce, ReinforceParams\n",
    "from algorithms.reinforce_runner import ReinforceRunner\n",
    "from algorithms.actorcritic import ActorCritic, ActorCriticParams\n",
    "from algorithms.actorcritic_runner import ActorCriticRunner\n",
    "\n",
    "import itertools\n",
    "\n",
    "class Manifest:\n",
    "    envs = [Env.CARTPOLE, Env.LUNARLANDER]\n",
    "    algos = [Reinforce, ActorCritic, DQN, DDQN]\n",
    "    algo_runners = {Reinforce: ReinforceRunner, ActorCritic: ActorCriticRunner, \n",
    "                    DQN: DQNRunner, DDQN: DDQNRunner}\n",
    "    algo_params = {Reinforce: ReinforceParams, ActorCritic: ActorCriticParams, \n",
    "                    DQN: DQNParams, DDQN: DDQNParams}\n",
    "    algo_name_dic = {'Reinforce': Reinforce, 'ActorCritic': ActorCritic, 'DQN': DQN, 'DDQN': DDQN}\n",
    "\n",
    "    short_dic = {\n",
    "        'train': 'train',\n",
    "        'intvl': 'check_interval',\n",
    "        'rwdscl': 'reward_scale',\n",
    "        'node': 'n_node',\n",
    "        'lRate': 'learning_rate',\n",
    "        'gma': 'gamma',\n",
    "        'nRoll': 'n_rollout',\n",
    "        'nBuf': 'buffer_limit',\n",
    "        'nBat': 'batch_size',\n",
    "        'nStrt': 'n_train_start',\n",
    "        'updIntvl': 'update_interval',\n",
    "        'lmb': 'lmbda',\n",
    "        'epsclp': 'eps_clip',\n",
    "        'k': 'k_epoch',\n",
    "        't': 't_horizon'\n",
    "    }\n",
    "    full_dic = {v: k for k, v in short_dic.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def get_algo_class(cls, algo_name):\n",
    "        return cls.algo_name_dic[algo_name]\n",
    "\n",
    "    @classmethod\n",
    "    def get_runner_class(cls, algo_class):\n",
    "        return cls.algo_runners[algo_class]\n",
    "\n",
    "    @classmethod\n",
    "    def get_param_class(cls, algo_class):\n",
    "        return cls.algo_params[algo_class]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_param_full(cls, short_param):\n",
    "        return cls.short_dic[short_param]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_param_short(cls, full_param):\n",
    "        return cls.full_dic[full_param]\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, check_intervals=None):\n",
    "        self._testcases = []\n",
    "        self._check_intervals = check_intervals\n",
    "        self.allcases = [*itertools.product(Manifest.envs, Manifest.algos)]\n",
    "    \n",
    "    def default_hyperparam(self, env, algo):\n",
    "        algo_param = None\n",
    "\n",
    "        if env == Env.CARTPOLE:\n",
    "            if algo == Reinforce:\n",
    "                algo_param = Manifest.get_param_class(algo)(\n",
    "                        n_node=32, learning_rate=0.0005, gamma=0.98)\n",
    "            elif algo == ActorCritic:\n",
    "                algo_param = Manifest.get_param_class(algo)(\n",
    "                        n_node=32, learning_rate=0.0005, gamma=0.98)\n",
    "            elif algo == DQN:\n",
    "                algo_param = Manifest.get_param_class(algo)(\n",
    "                        n_node=32, learning_rate=0.0005, gamma=0.98, buffer_limit=50000, \n",
    "                        batch_size=32, n_train_start=2000, start_epsilon=0.1, update_interval=10)\n",
    "            elif algo == DDQN:\n",
    "                algo_param = Manifest.get_param_class(algo)(\n",
    "                        n_node=32, learning_rate=0.0005, gamma=0.98, buffer_limit=50000, \n",
    "                        batch_size=32, n_train_start=2000, start_epsilon=0.1, update_interval=10)\n",
    "            else:\n",
    "                raise Exception(f'algorithm does not exist: {algo}')\n",
    "        elif env == Env.LUNARLANDER:\n",
    "            if algo == Reinforce:\n",
    "                algo_param = Manifest.get_param_class(algo)(\n",
    "                        n_node=128, learning_rate=0.0025, gamma=0.98)\n",
    "            elif algo == ActorCritic:\n",
    "                algo_param = Manifest.get_param_class(algo)(\n",
    "                        n_node=128, learning_rate=0.0025, gamma=0.98)\n",
    "            elif algo == DQN:\n",
    "                algo_param = Manifest.get_param_class(algo)(\n",
    "                        n_node=128, learning_rate=0.0025, gamma=0.98, buffer_limit=100000, \n",
    "                        batch_size=64, n_train_start=10000, start_epsilon=0.2, update_interval=20)\n",
    "            elif algo == DDQN:\n",
    "                algo_param = Manifest.get_param_class(algo)(\n",
    "                        n_node=128, learning_rate=0.0025, gamma=0.98, buffer_limit=100000, \n",
    "                        batch_size=64, n_train_start=10000, start_epsilon=0.2, update_interval=20)\n",
    "            else:\n",
    "                raise Exception(f'algorithm does not exist: {algo}')\n",
    "        else:\n",
    "            raise Exception(f'env does not exist: {env}')\n",
    "\n",
    "        return algo_param\n",
    "\n",
    "    def add_case(self, env, algo, algo_param_dic=None):\n",
    "        if not algo_param_dic:\n",
    "            algo_param_dic = dict()\n",
    "\n",
    "        algo_param_class = Manifest.get_param_class(algo)\n",
    "        algo_param_dic = {**self.default_hyperparam(env, algo).__dict__, **algo_param_dic}\n",
    "        algo_param = algo_param_class(**algo_param_dic)\n",
    "        algo_runner = Manifest.get_runner_class(algo)\n",
    "\n",
    "        self._testcases += [(env, algo_runner, algo_param)]\n",
    "\n",
    "    def run(self, runner_param_dic=None, debug=False):\n",
    "        for check_interval in self._check_intervals:\n",
    "            runnerp = None\n",
    "            if debug:\n",
    "                runnerp = RunnerParams(save_net=True, max_video=1000, video_record_interval=self._check_intervals, \n",
    "                                            print_interval=self._check_intervals)\n",
    "            elif runner_param_dic == None:\n",
    "                runnerp = RunnerParams(save_net=True, video_record_interval=0, print_interval=0)\n",
    "            else:\n",
    "                runnerp = RunnerParams(**runner_param_dic)\n",
    "\n",
    "            runnerp.check_interval = check_interval\n",
    "\n",
    "            for i, (env, runner, algop) in enumerate(self._testcases):\n",
    "                runnerp.name_postfix=str(algop)\n",
    "\n",
    "                if env == Env.CARTPOLE:\n",
    "                    runnerp.target_score = 500.0\n",
    "                    runnerp.reward_scale = 100.0\n",
    "                elif env == Env.LUNARLANDER:\n",
    "                    runnerp.target_score = 200.0\n",
    "                    runnerp.reward_scale = 30.0\n",
    "\n",
    "                runner(env.value, algop, runnerp).run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "시뮬레이션 시작\n"
     ]
    }
   ],
   "source": [
    "class player:\n",
    "\n",
    "    def __init__(self, path, model_names):\n",
    "        self.path = path\n",
    "        self.model_names = model_names\n",
    "\n",
    "    def run(self, debug=False):\n",
    "        cases = []\n",
    "        for name in self.model_names:\n",
    "            tokens = name.split('_')\n",
    "            load_name = name + \".pt\"\n",
    "            algo_name, env_name, last_score = tokens[0:3]\n",
    "            train, check_interval, reward_scale = tokens[3:6]\n",
    "            algo_params = dict()\n",
    "            for token in tokens[6:-1]:\n",
    "                k, v = token.split('=')\n",
    "                k = Manifest.get_param_full(k)\n",
    "                try: algo_params[k] = int(v)\n",
    "                except: algo_params[k] = float(v)\n",
    "            algo = Manifest.get_algo_class(algo_name)\n",
    "            algop = Manifest.get_param_class(algo)(**algo_params)\n",
    "            runner = Manifest.get_runner_class(algo)\n",
    "            cases.append((env_name, runner, algop, algo_params, load_name))\n",
    "\n",
    "        for env, runner, algop, algo_params, load_name in cases:\n",
    "            print(f'\\t[ {env}, {runner} ]\\n parameters {algo_params.items()}\\n')\n",
    "            runnerp = None\n",
    "            if debug:\n",
    "                runnerp = RunnerParams(train=False, save_net=False, load_net=True, target_score=9999.0,\n",
    "                                        load_name=load_name, name_postfix=str(algop), \n",
    "                                        check_interval=1, max_video=3, save_check_log=False, save_step_log=True,\n",
    "                                        print_interval=1, video_record_interval=1, max_episode=1000)\n",
    "            else:\n",
    "                runnerp = RunnerParams(train=False, save_net=False, load_net=True, target_score=9999.0,\n",
    "                                        load_name=load_name, name_postfix=str(algop), \n",
    "                                        check_interval=1, max_video=0, save_check_log=False, save_step_log=True,\n",
    "                                        print_interval=0, video_record_interval=0, max_episode=1000)\n",
    "\n",
    "            runner(env, algop, runnerp).run()\n",
    "        \n",
    "        print('모두 종료됨')\n",
    "\n",
    "\n",
    "def train():\n",
    "    \n",
    "    all_cases = Trainer().allcases\n",
    "    cartpole_cases = filter(lambda x:x[0] == Env.CARTPOLE, all_cases)\n",
    "    lunar_cases = filter(lambda x:x[0] == Env.LUNARLANDER, all_cases)\n",
    "    \n",
    "    check_interval_arr = [10]\n",
    "    tr = Trainer(check_interval_arr)\n",
    "\n",
    "    for env, algo in cartpole_cases:\n",
    "        tr.add_case(env, algo)\n",
    "    tr.run()\n",
    "\n",
    "    check_interval_arr = [5]\n",
    "    tr = Trainer(check_interval_arr)\n",
    "\n",
    "    for env, algo in lunar_cases:\n",
    "        tr.add_case(env, algo)\n",
    "    tr.run()\n",
    "\n",
    "def replay():\n",
    "    model_names = [\n",
    "        'Reinforce_LunarLander-v2_204_train=True_intvl=10_rwdscl=30.0_node=128_lRate=0.0025_gma=0.98_1635200309',\n",
    "        'ActorCritic_LunarLander-v2_217_train=True_intvl=10_rwdscl=30.0_node=128_lRate=0.0025_gma=0.98_1635202089',\n",
    "        'DQN_LunarLander-v2_200_train=True_intvl=10_rwdscl=30.0_node=128_lRate=0.0025_gma=0.98_nBuf=100000_nBat=64_nStrt=10000_updIntvl=20_1635208616',\n",
    "        'DDQN_LunarLander-v2_215_train=True_intvl=10_rwdscl=30.0_node=128_lRate=0.0025_gma=0.98_nBuf=100000_nBat=64_nStrt=10000_updIntvl=20_1635209424'\n",
    "    ]\n",
    "    player('weights', model_names).run(debug=True)\n",
    "\n",
    "# Remover().remove_dirs(['runs', 'weights', 'videos'])\n",
    "# Remover().remove_dirs(['logs'])\n",
    "train()\n",
    "\n",
    "# print_interval=10, max_epi=100 > 4분 35초\n",
    "# print_interval=0, max_epi=100 > 4분 25초\n",
    "# save_check_log=True, print_interval=0, max_epi=100 > 4분 37초\n",
    "\n",
    "# for _ in range(10):\n",
    "#     replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_reinforce():\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.LUNARLANDER, Reinforce)]:\n",
    "        start = 5\n",
    "        k = 5\n",
    "        for i in range(21):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.learning_rate = (start + k*i)/10000\n",
    "            tr.add_case(env, algo, hparam)\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_actorcritic():\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.LUNARLANDER, ActorCritic)]:\n",
    "        start = 1\n",
    "        k = 5\n",
    "        for i in range(21):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.learning_rate = (start + k*i)/10000\n",
    "            tr.add_case(env, algo, hparam)\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_dqn_node_cartpole():\n",
    "    \"\"\"\n",
    "    8\n",
    "    \"\"\"\n",
    "    tr = Trainer([20])\n",
    "    for env, algo in [(Env.CARTPOLE, DQN)]:\n",
    "        n_node = 2\n",
    "        for i in range(8):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.n_node = n_node\n",
    "            tr.add_case(env, algo, hparam)\n",
    "            n_node *= 2\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_dqn_node_lunarlander():\n",
    "    \"\"\"\n",
    "    64~256\n",
    "    \"\"\"\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.LUNARLANDER, DQN)]:\n",
    "        n_node = 2\n",
    "        for i in range(8):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.n_node = n_node\n",
    "            tr.add_case(env, algo, hparam)\n",
    "            n_node *= 2\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_ddqn_node_cartpole():\n",
    "    \"\"\"\n",
    "    8~16\n",
    "    \"\"\"\n",
    "    tr = Trainer([20])\n",
    "    for env, algo in [(Env.CARTPOLE, DDQN)]:\n",
    "        n_node = 64\n",
    "        for i in range(2):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.n_node = n_node\n",
    "            tr.add_case(env, algo, hparam)\n",
    "            n_node *= 2\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_ddqn_node_lunarlander():\n",
    "    \"\"\"\n",
    "    32, 128, 256\n",
    "    \"\"\"\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.LUNARLANDER, DDQN)]:\n",
    "        n_node = 32\n",
    "        for i in range(4):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.n_node = n_node\n",
    "            tr.add_case(env, algo, hparam)\n",
    "            n_node *= 2\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "# Remover().remove_dirs(['runs', 'weights', 'videos'])\n",
    "# tuning_dqn_node_cartpole()\n",
    "# tuning_dqn_node_lunarlander()\n",
    "tuning_ddqn_node_lunarlander()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92bb61e8f2b0dcdaa21cd71aad1d97e3da046ad8a677ac321cbd25d595832889"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('py38-pytorch-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
