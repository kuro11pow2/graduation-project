{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrun tensorboard server\\n\\nconda activate py38-pytorch-gpu && tensorboard --port=6006 --logdir=runs\\nhttp://localhost:6006/\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "run anaconda prompt \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "home_path = os.path.expanduser('~')\n",
    "cur_path = os.getcwd()\n",
    "conda_path = home_path + \"\\\\anaconda3\"\n",
    "conda_script_path = home_path + \"\\\\anaconda3\\\\Scripts\\\\activate.bat\"\n",
    "exc = ' '.join(['start', '%windir%\\System32\\cmd.exe \"/K\"', conda_script_path, conda_path])\n",
    "!$exc\n",
    "\n",
    "\"\"\"\n",
    "run tensorboard server\n",
    "\n",
    "conda activate py38-pytorch-gpu && tensorboard --port=6006 --logdir=runs\n",
    "http://localhost:6006/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgoParamDict:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        만드는 중\n",
    "        \"\"\"\n",
    "        self.dic = {\n",
    "            'rwdscl': 'reward_scale'\n",
    "        }\n",
    "        self.r_dic = {v: k for k, v in self.dic.items()}\n",
    "    \n",
    "    def long(self, s):\n",
    "        return self.dic.get(s, None)\n",
    "        \n",
    "    def short(self, s):\n",
    "        return self.r_dic.get(s, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Env\n",
    "from runner import RunnerParams\n",
    "from logger import Logger\n",
    "from remover import Remover\n",
    "from algorithms.dqn import DQN, DQNParams\n",
    "from algorithms.dqn_runner import DQNRunner\n",
    "from algorithms.ddqn import DDQN, DDQNParams\n",
    "from algorithms.ddqn_runner import DDQNRunner\n",
    "from algorithms.reinforce import Reinforce, ReinforceParams\n",
    "from algorithms.reinforce_runner import ReinforceRunner\n",
    "from algorithms.actorcritic import ActorCritic, ActorCriticParams\n",
    "from algorithms.actorcritic_runner import ActorCriticRunner\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, check_intervals=None):\n",
    "        self._testcases = []\n",
    "        self._envs = [Env.CARTPOLE, Env.LUNARLANDER]\n",
    "        self._algos = [Reinforce, ActorCritic, DQN, DDQN]\n",
    "        self._runners = {Reinforce: ReinforceRunner, ActorCritic: ActorCriticRunner, \n",
    "                        DQN: DQNRunner, DDQN: DDQNRunner}\n",
    "        self._params = {Reinforce: ReinforceParams, ActorCritic: ActorCriticParams, \n",
    "                        DQN: DQNParams, DDQN: DDQNParams}\n",
    "        self._check_intervals = check_intervals\n",
    "        self.allcases = [*itertools.product(self._envs, self._algos)]\n",
    "    \n",
    "    def default_hyperparam(self, env, algo):\n",
    "        algo_param = None\n",
    "\n",
    "        if env == Env.CARTPOLE:\n",
    "            if algo == Reinforce:\n",
    "                algo_param = self._params[algo](\n",
    "                        n_node=128, learning_rate=0.0005, gamma=0.98)\n",
    "            elif algo == ActorCritic:\n",
    "                algo_param = self._params[algo](\n",
    "                        n_node=128, learning_rate=0.0005, gamma=0.98, n_rollout=10)\n",
    "            elif algo == DQN:\n",
    "                algo_param = self._params[algo](\n",
    "                        n_node=128, learning_rate=0.0005, gamma=0.98, buffer_limit=50000, \n",
    "                        batch_size=32, n_train_start=2000, start_epsilon=0.1, update_interval=10)\n",
    "            elif algo == DDQN:\n",
    "                algo_param = self._params[algo](\n",
    "                        n_node=128, learning_rate=0.0005, gamma=0.98, buffer_limit=50000, \n",
    "                        batch_size=32, n_train_start=2000, start_epsilon=0.1, update_interval=10)\n",
    "            else:\n",
    "                raise Exception(f'algorithm does not exist: {algo}')\n",
    "        elif env == Env.LUNARLANDER:\n",
    "            if algo == Reinforce:\n",
    "                algo_param = self._params[algo](\n",
    "                        n_node=256, learning_rate=0.0025, gamma=0.98)\n",
    "            elif algo == ActorCritic:\n",
    "                algo_param = self._params[algo](\n",
    "                        n_node=256, learning_rate=0.0025, gamma=0.98, n_rollout=20)\n",
    "            elif algo == DQN:\n",
    "                algo_param = self._params[algo](\n",
    "                        n_node=256, learning_rate=0.0005, gamma=0.98, buffer_limit=100000, \n",
    "                        batch_size=64, n_train_start=10000, start_epsilon=0.2, update_interval=20)\n",
    "            elif algo == DDQN:\n",
    "                algo_param = self._params[algo](\n",
    "                        n_node=256, learning_rate=0.0005, gamma=0.98, buffer_limit=100000, \n",
    "                        batch_size=64, n_train_start=10000, start_epsilon=0.2, update_interval=20)\n",
    "            else:\n",
    "                raise Exception(f'algorithm does not exist: {algo}')\n",
    "        else:\n",
    "            raise Exception(f'env does not exist: {env}')\n",
    "\n",
    "        return algo_param\n",
    "\n",
    "    def add_case(self, env, algo, algo_param=None):\n",
    "        algo_param = algo_param if algo_param else self.default_hyperparam(env, algo)\n",
    "        algo_runner = self._runners[algo]\n",
    "        self._testcases += [(env, algo_runner, algo_param)]\n",
    "\n",
    "    def run(self, runner_params=dict()):\n",
    "        runner_params = {**runner_params, \n",
    "                        'save_net':True, 'video_record_interval':0, 'print_interval':0}\n",
    "        runner_param = RunnerParams(**runner_params)\n",
    "\n",
    "        for check_interval in self._check_intervals:\n",
    "            runner_param.check_interval = check_interval\n",
    "\n",
    "            for i, (env, runner, algo_param) in enumerate(self._testcases):\n",
    "                runner_param.name_postfix=str(algo_param)\n",
    "\n",
    "                if env == Env.CARTPOLE:\n",
    "                    runner_param.target_score = 500.0\n",
    "                    runner_param.reward_scale = 100.0\n",
    "                elif env == Env.LUNARLANDER:\n",
    "                    runner_param.target_score = 200.0\n",
    "                    runner_param.reward_scale = 30.0\n",
    "\n",
    "                runner(env.value, algo_param, runner_param).run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kuro1\\Source\\Repos\\Remote\\Univ\\graduation-project\\src\\logs 처리중 에러 발생\n",
      "<class 'FileNotFoundError'>\n",
      "(2, '지정된 경로를 찾을 수 없습니다')\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: Reinforce\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: ActorCritic\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: LunarLander-v2\n",
      "state space: (8,)\n",
      "action space: Discrete(4)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "시뮬레이션 종료\n",
      "1000 에피소드 초과하여 종료\n",
      "모두 종료됨\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    all_cases = Trainer().allcases\n",
    "    cartpole_cases = filter(lambda x:x[0] == Env.CARTPOLE, all_cases)\n",
    "    lunar_cases = filter(lambda x:x[0] == Env.LUNARLANDER, all_cases)\n",
    "\n",
    "    # tr = Trainer([20])\n",
    "    # for env, algo in cartpole_cases:\n",
    "    #     tr.add_case(env, algo)\n",
    "    # tr.run()\n",
    "\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in lunar_cases:\n",
    "        tr.add_case(env, algo)\n",
    "    tr.run()\n",
    "\n",
    "def replay():\n",
    "    cases = [\n",
    "        [\n",
    "            Env.CARTPOLE.value, \n",
    "            ReinforceRunner,\n",
    "            ReinforceParams(n_node=128, learning_rate=0.0005, gamma=0.98),\n",
    "            \"Reinforce_CartPole-v1_500_train=True_intvl=20_rwdscl=100.0_node=128_lRate=0.0005_gma=0.98_1634893243\"\n",
    "        ],\n",
    "        [\n",
    "            Env.CARTPOLE.value, \n",
    "            ActorCriticRunner,\n",
    "            ActorCriticParams(n_node=128, learning_rate=0.0005, gamma=0.98, n_rollout=10),\n",
    "            \"ActorCritic_CartPole-v1_500_train=True_intvl=20_rwdscl=100.0_node=128_lRate=0.0005_gma=0.98_nRoll=10_1634893506\"\n",
    "        ],\n",
    "        [\n",
    "            Env.CARTPOLE.value, \n",
    "            DQNRunner,\n",
    "            DQNParams(n_node=128, learning_rate=0.0005, gamma=0.98, buffer_limit=50000, \n",
    "                        batch_size=32, n_train_start=2000, update_interval=10),\n",
    "            \"DQN_CartPole-v1_500_train=True_intvl=20_rwdscl=100.0_node=128_lRate=0.0005_gma=0.98_nBuf=50000_nBat=32_nStrt=2000_updIntvl=10_1634893791\"\n",
    "        ],\n",
    "        [\n",
    "            Env.CARTPOLE.value, \n",
    "            DDQNRunner,\n",
    "            DDQNParams(n_node=128, learning_rate=0.0005, gamma=0.98, buffer_limit=50000, \n",
    "                        batch_size=32, n_train_start=5000, update_interval=10),\n",
    "            \"DDQN_CartPole-v1_500_train=True_intvl=20_rwdscl=100.0_node=128_lRate=0.0005_gma=0.98_nBuf=50000_nBat=32_nStrt=2000_updIntvl=10_1634893884\"\n",
    "        ],\n",
    "        [\n",
    "            Env.LUNARLANDER.value, \n",
    "            ReinforceRunner,\n",
    "            ReinforceParams(n_node=256, learning_rate=0.0025, gamma=0.98),\n",
    "            \"Reinforce_LunarLander-v2_209_train=True_intvl=10_rwdscl=30.0_node=256_lRate=0.0025_gma=0.98_1634900723\"\n",
    "        ],\n",
    "        [\n",
    "            Env.LUNARLANDER.value, \n",
    "            ActorCriticRunner,\n",
    "            ActorCriticParams(n_node=256, learning_rate=0.0025, gamma=0.98, n_rollout=20),\n",
    "            \"ActorCritic_LunarLander-v2_209_train=True_intvl=10_rwdscl=30.0_node=256_lRate=0.0025_gma=0.98_nRoll=20_1634902832\"\n",
    "        ],\n",
    "        [\n",
    "            Env.LUNARLANDER.value, \n",
    "            DQNRunner,\n",
    "            DQNParams(n_node=256, learning_rate=0.0005, gamma=0.98, buffer_limit=100000, \n",
    "                        batch_size=64, n_train_start=10000, update_interval=20),\n",
    "            \"DQN_LunarLander-v2_226_train=True_intvl=10_rwdscl=30.0_node=256_lRate=0.0005_gma=0.98_nBuf=100000_nBat=64_nStrt=10000_updIntvl=20_1634905473\"\n",
    "        ],\n",
    "        [\n",
    "            Env.LUNARLANDER.value, \n",
    "            DDQNRunner,\n",
    "            DDQNParams(n_node=256, learning_rate=0.0005, gamma=0.98, buffer_limit=100000, \n",
    "                        batch_size=64, n_train_start=10000, update_interval=20),\n",
    "            \"DDQN_LunarLander-v2_220_train=True_intvl=10_rwdscl=30.0_node=256_lRate=0.0005_gma=0.98_nBuf=100000_nBat=64_nStrt=10000_updIntvl=20_1634906356\"\n",
    "        ],\n",
    "    ]\n",
    "    \n",
    "    for case in cases:\n",
    "        case[-1] += \".pt\"\n",
    "    \n",
    "    for env, runner, algop, load_name in cases:\n",
    "        runnerp = RunnerParams(train=False, save_net=False, load_net=True, target_score=9999.0,\n",
    "                                load_name=load_name, name_postfix=str(algop), \n",
    "                                check_interval=1, max_video=0, save_check_log=False, save_step_log=True,\n",
    "                                print_interval=0, video_record_interval=0, max_episode=1000)\n",
    "        runner(env, algop, runnerp).run()\n",
    "    \n",
    "    print('모두 종료됨')\n",
    "\n",
    "# Remover().remove_dirs(['runs', 'weights', 'videos'])\n",
    "# Remover().remove_dirs(['videos'])\n",
    "# print_interval=10, max_epi=100 > 4분 35초\n",
    "# print_interval=0, max_epi=100 > 4분 25초\n",
    "# save_check_log=True, print_interval=0, max_epi=100 > 4분 37초\n",
    "Remover().remove_dirs(['logs'])\n",
    "for _ in range(10):\n",
    "    replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_reinforce():\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.LUNARLANDER, Reinforce)]:\n",
    "        start = 5\n",
    "        k = 5\n",
    "        for i in range(21):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.learning_rate = (start + k*i)/10000\n",
    "            tr.add_case(env, algo, hparam)\n",
    "\n",
    "    runner_params = {'max_episode':1000}\n",
    "    tr.run(runner_params)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_ddqn():\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.CARTPOLE, DDQN), (Env.LUNARLANDER, DDQN)]:\n",
    "        n_node = 16\n",
    "        for i in range(5):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.n_node = n_node\n",
    "            tr.add_case(env, algo, hparam)\n",
    "            n_node *= 2\n",
    "\n",
    "    runner_params = {'max_episode':1000}\n",
    "    tr.run(runner_params)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "# Remover().remove_dirs(['runs', 'weights', 'videos'])\n",
    "tuning_ddqn()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92bb61e8f2b0dcdaa21cd71aad1d97e3da046ad8a677ac321cbd25d595832889"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('py38-pytorch-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
