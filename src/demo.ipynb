{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "run anaconda prompt \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "home_path = os.path.expanduser('~')\n",
    "cur_path = os.getcwd()\n",
    "conda_path = home_path + \"\\\\anaconda3\"\n",
    "conda_script_path = home_path + \"\\\\anaconda3\\\\Scripts\\\\activate.bat\"\n",
    "exc = ' '.join(['start', '%windir%\\System32\\cmd.exe \"/K\"', conda_script_path, conda_path])\n",
    "!$exc\n",
    "\n",
    "\"\"\"\n",
    "run tensorboard server\n",
    "\n",
    "conda activate py38-pytorch-gpu && tensorboard --port=6006 --logdir=runs\n",
    "http://localhost:6006/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DQN_CartPole-v1_500_train=True_intvl=10_rwdscl=100.0_node=32_lRate=0.0005_gma=0.98_nBuf=50000_nBat=32_nStrt=2000_updIntvl=10_1635236152', 'DDQN_CartPole-v1_500_train=True_intvl=10_rwdscl=100.0_node=32_lRate=0.0005_gma=0.98_nBuf=50000_nBat=32_nStrt=2000_updIntvl=10_1635236310']\n",
      "\t[ CartPole-v1, <class 'algorithms.dqn_runner.DQNRunner'> ]\n",
      " parameters dict_items([('n_node', 32), ('learning_rate', 0.0005), ('gamma', 0.98), ('buffer_limit', 50000), ('batch_size', 32), ('n_train_start', 2000), ('update_interval', 10)])\n",
      "\n",
      "초기 설정\n",
      "algorithm: DQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "n_epi=0, self._score=428.0 비디오 저장\n",
      "에피소드: 0, 점수: 428.0\n",
      "n_buffer : 0, eps : 0.0%\n",
      "n_epi=1, self._score=496.0 비디오 저장\n",
      "에피소드: 1, 점수: 496.0\n",
      "n_buffer : 0, eps : 0.0%\n",
      "n_epi=2, self._score=500.0 비디오 저장\n",
      "에피소드: 2, 점수: 500.0\n",
      "n_buffer : 0, eps : 0.0%\n",
      "종료 조건 만족. 최종 1번 평균 점수 500.0\n",
      "\t[ CartPole-v1, <class 'algorithms.ddqn_runner.DDQNRunner'> ]\n",
      " parameters dict_items([('n_node', 32), ('learning_rate', 0.0005), ('gamma', 0.98), ('buffer_limit', 50000), ('batch_size', 32), ('n_train_start', 2000), ('update_interval', 10)])\n",
      "\n",
      "초기 설정\n",
      "algorithm: DDQN\n",
      "env: CartPole-v1\n",
      "state space: (4,)\n",
      "action space: Discrete(2)\n",
      "네트워크 불러오기\n",
      "시뮬레이션 시작\n",
      "n_epi=0, self._score=500.0 비디오 저장\n",
      "에피소드: 0, 점수: 500.0\n",
      "n_buffer : 0, eps : 0.0%\n",
      "n_epi=1, self._score=500.0 비디오 저장\n",
      "에피소드: 1, 점수: 500.0\n",
      "n_buffer : 0, eps : 0.0%\n",
      "n_epi=2, self._score=500.0 비디오 저장\n",
      "에피소드: 2, 점수: 500.0\n",
      "n_buffer : 0, eps : 0.0%\n",
      "종료 조건 만족. 최종 1번 평균 점수 500.0\n",
      "모두 종료됨\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from manifest import Manifest\n",
    "from logger import Logger\n",
    "from remover import Remover\n",
    "\n",
    "# 자동완성용 import\n",
    "from env import Env\n",
    "from runner import RunnerParams\n",
    "from trainer import Trainer\n",
    "from player import Player\n",
    "\n",
    "from algorithms.dqn import DQN, DQNParams\n",
    "from algorithms.dqn_runner import DQNRunner\n",
    "from algorithms.ddqn import DDQN, DDQNParams\n",
    "from algorithms.ddqn_runner import DDQNRunner\n",
    "from algorithms.reinforce import Reinforce, ReinforceParams\n",
    "from algorithms.reinforce_runner import ReinforceRunner\n",
    "from algorithms.actorcritic import ActorCritic, ActorCriticParams\n",
    "from algorithms.actorcritic_runner import ActorCriticRunner\n",
    "\n",
    "\n",
    "import collections\n",
    "\n",
    "def train():\n",
    "    cases = Trainer().allcases\n",
    "    cartpole_cases = filter(lambda x:x[0] == Env.CARTPOLE, cases)\n",
    "    lunar_cases = filter(lambda x:x[0] == Env.LUNARLANDER, cases)\n",
    "\n",
    "    check_interval_arr = [5]\n",
    "    tr = Trainer(check_interval_arr)\n",
    "    for env, algo in lunar_cases:\n",
    "        tr.add_case(env, algo)\n",
    "    tr.run()\n",
    "    \n",
    "    check_interval_arr = [10]\n",
    "    tr = Trainer(check_interval_arr)\n",
    "    for env, algo in cartpole_cases:\n",
    "        tr.add_case(env, algo)\n",
    "    tr.run()\n",
    "\n",
    "\n",
    "def train_dqn_ddqn():\n",
    "    cases = Trainer().allcases\n",
    "    target_cases = filter(lambda x:(x[1] == DQN) or (x[1] == DDQN), cases)\n",
    "    cartpole_cases = filter(lambda x:x[0] == Env.CARTPOLE, target_cases)\n",
    "    lunar_cases = filter(lambda x:x[0] == Env.LUNARLANDER, target_cases)\n",
    "\n",
    "    check_interval_arr = [5]\n",
    "    tr = Trainer(check_interval_arr)\n",
    "    for env, algo in lunar_cases:\n",
    "        tr.add_case(env, algo)\n",
    "    tr.run()\n",
    "    \n",
    "    # check_interval_arr = [10]\n",
    "    # tr = Trainer(check_interval_arr)\n",
    "    # for env, algo in cartpole_cases:\n",
    "    #     tr.add_case(env, algo)\n",
    "    # tr.run()\n",
    "    \n",
    "\n",
    "def play():\n",
    "    PATH = os.path.join(os.getcwd(), 'weights')\n",
    "    filenames = ['.'.join(tokens[:-1]) \n",
    "                        for name in os.listdir(PATH) \n",
    "                        if (tokens := name.split('.')) and len(tokens) > 2]\n",
    "\n",
    "    Player('weights', filenames).run(runner_param_dic={'max_episode': 125})\n",
    "\n",
    "\n",
    "def debug_play():\n",
    "    PATH = os.path.join(os.getcwd(), 'weights')\n",
    "    filenames = ['.'.join(tokens[:-1]) \n",
    "                        for name in os.listdir(PATH) \n",
    "                        if (tokens := name.split('.')) and len(tokens) > 2]\n",
    "    tmp1 = list(filter(lambda x: x.startswith('DQN'), filenames))\n",
    "    tmp2 = list(filter(lambda x: x.startswith('DDQN'), filenames))\n",
    "    filenames = tmp1[:1] + tmp2[:1]\n",
    "    print(filenames)\n",
    "    Player('weights', filenames).run(debug=True)\n",
    "\n",
    "\n",
    "\n",
    "# Remover().remove_dirs(['runs', 'weights', 'videos'])\n",
    "# Remover().remove_dirs(['logs'])\n",
    "\n",
    "# for i in range(10):\n",
    "#     train()\n",
    "\n",
    "# print_interval=10, max_epi=100 > 4분 35초\n",
    "# print_interval=0, max_epi=100 > 4분 25초\n",
    "# save_check_log=True, print_interval=0, max_epi=100 > 4분 37초\n",
    "\n",
    "debug_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_reinforce():\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.LUNARLANDER, Reinforce)]:\n",
    "        start = 5\n",
    "        k = 5\n",
    "        for i in range(21):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.learning_rate = (start + k*i)/10000\n",
    "            tr.add_case(env, algo, hparam)\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_actorcritic():\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.LUNARLANDER, ActorCritic)]:\n",
    "        start = 1\n",
    "        k = 5\n",
    "        for i in range(21):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.learning_rate = (start + k*i)/10000\n",
    "            tr.add_case(env, algo, hparam)\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_dqn_node_cartpole():\n",
    "    \"\"\"\n",
    "    8\n",
    "    \"\"\"\n",
    "    tr = Trainer([20])\n",
    "    for env, algo in [(Env.CARTPOLE, DQN)]:\n",
    "        n_node = 2\n",
    "        for i in range(8):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.n_node = n_node\n",
    "            tr.add_case(env, algo, hparam)\n",
    "            n_node *= 2\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_dqn_node_lunarlander():\n",
    "    \"\"\"\n",
    "    64~256\n",
    "    \"\"\"\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.LUNARLANDER, DQN)]:\n",
    "        n_node = 2\n",
    "        for i in range(8):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.n_node = n_node\n",
    "            tr.add_case(env, algo, hparam)\n",
    "            n_node *= 2\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_ddqn_node_cartpole():\n",
    "    \"\"\"\n",
    "    8~16\n",
    "    \"\"\"\n",
    "    tr = Trainer([20])\n",
    "    for env, algo in [(Env.CARTPOLE, DDQN)]:\n",
    "        n_node = 64\n",
    "        for i in range(2):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.n_node = n_node\n",
    "            tr.add_case(env, algo, hparam)\n",
    "            n_node *= 2\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "def tuning_ddqn_node_lunarlander():\n",
    "    \"\"\"\n",
    "    32, 128, 256\n",
    "    \"\"\"\n",
    "    tr = Trainer([10])\n",
    "    for env, algo in [(Env.LUNARLANDER, DDQN)]:\n",
    "        n_node = 32\n",
    "        for i in range(4):\n",
    "            hparam = tr.default_hyperparam(env, algo)\n",
    "            hparam.n_node = n_node\n",
    "            tr.add_case(env, algo, hparam)\n",
    "            n_node *= 2\n",
    "\n",
    "    runner_param_dic = {'save_net':True, 'max_episode':10000, 'print_interval':0, 'video_record_interval':0}\n",
    "    tr.run(runner_param_dic)\n",
    "    print('전체 테스트 종료')\n",
    "\n",
    "# Remover().remove_dirs(['runs', 'weights', 'videos'])\n",
    "# tuning_dqn_node_cartpole()\n",
    "# tuning_dqn_node_lunarlander()\n",
    "tuning_ddqn_node_lunarlander()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92bb61e8f2b0dcdaa21cd71aad1d97e3da046ad8a677ac321cbd25d595832889"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('py38-pytorch-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
